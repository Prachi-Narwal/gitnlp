{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b2wDgRDQExoS",
        "P1MSrnZtO3Ok",
        "MHsrP4YIudOh",
        "JFnxlwsq5XQt",
        "krOIhyFbNuAK",
        "_PVkdab3SuVt"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnbkbYwEcHevyz8acLeT+o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenization"
      ],
      "metadata": {
        "id": "b2wDgRDQExoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJINF9jhEQBD",
        "outputId": "d1d833b4-1616-4c0d-c47f-24e1d3a410c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "corpus=\"\"\"Hello! I like to eat Guava, grapes.\n",
        "Meena's friends also like Guava.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUe4y3_REgVu",
        "outputId": "018a332c-2a4e-47dc-f5b2-c521276b8c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d01IRum5Fqg8",
        "outputId": "25503d9b-7082-41ae-a41f-d38500dbcc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I like to eat Guava, grapes.\n",
            "Meena's friends also like Guava.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Paragraph --> sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents=sent_tokenize(corpus)\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y64tTa8Fsxr",
        "outputId": "4b4d40ea-cb4c-40b0-b49c-fa0055fcdc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello!', 'I like to eat Guava, grapes.', \"Meena's friends also like Guava.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8x9xPOuGsui",
        "outputId": "c9315574-c0f6-412e-a56c-2fcb2ead6a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSWJjfgoI2TT",
        "outputId": "0ee85476-c9c4-4772-ea40-d09f95852c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!\n",
            "I like to eat Guava, grapes.\n",
            "Meena's friends also like Guava.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# paragraph --> words\n",
        "\n",
        "from nltk.tokenize import word_tokenize   # 's is taken as one\n",
        "words=word_tokenize(corpus)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn5t2JmyI50j",
        "outputId": "e3310022-0060-4e49-bb33-a5466b016eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.', 'Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence --> words\n",
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wZSj2FxJgR1",
        "outputId": "a2eded51-1c62-46a7-b885-7e550b6133ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!']\n",
            "['I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.']\n",
            "['Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import wordpunct_tokenize # 's is taken as 2\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GshgJ232KZMG",
        "outputId": "1b80b179-44a2-4cb0-a9ee-efc240b394ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " 'like',\n",
              " 'to',\n",
              " 'eat',\n",
              " 'Guava',\n",
              " ',',\n",
              " 'grapes',\n",
              " '.',\n",
              " 'Meena',\n",
              " \"'\",\n",
              " 's',\n",
              " 'friends',\n",
              " 'also',\n",
              " 'like',\n",
              " 'Guava',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TreebankWordTokenizer  # . is not treated separately, only last . is treated seprately\n",
        "tokenizer=TreebankWordTokenizer()  # it is a class\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxNqXgnYKf3_",
        "outputId": "a6d5a426-29f3-4f6e-9496-f12577a97de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " 'like',\n",
              " 'to',\n",
              " 'eat',\n",
              " 'Guava',\n",
              " ',',\n",
              " 'grapes.',\n",
              " 'Meena',\n",
              " \"'s\",\n",
              " 'friends',\n",
              " 'also',\n",
              " 'like',\n",
              " 'Guava',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Stemming"
      ],
      "metadata": {
        "id": "P1MSrnZtO3Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "2juA56aLMxWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=['Eating','eats','eat','ate','Adjustable','rafting','ability','meeting','history']"
      ],
      "metadata": {
        "id": "dRTyt2ytdarl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,'|',stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGtGrhMZj2JC",
        "outputId": "8e2e90cc-d645-41b9-c424-71c9d6ba0e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | ate\n",
            "Adjustable | adjust\n",
            "rafting | raft\n",
            "ability | abil\n",
            "meeting | meet\n",
            "history | histori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  disadvantage\n",
        "print(stemming.stem('history'))\n",
        "print(stemming.stem('congratulation'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoza9f-3j90Z",
        "outputId": "a3c81035-020e-4959-87ef-00194bf4cb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "histori\n",
            "congratul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  RegexStemmer class\n",
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
        "print(reg_stemmer.stem('eating'))\n",
        "print(reg_stemmer.stem('ingeating'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBSKdnaHkz5I",
        "outputId": "c055bfc4-7a54-47c0-dd03-091a4127f5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "ingeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer2=RegexpStemmer('ing|s$|e$|able$',min=4)\n",
        "print(reg_stemmer2.stem('eating'))\n",
        "print(reg_stemmer2.stem('ingeating'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPRnX2xbpfsT",
        "outputId": "08863e8a-5be6-47de-fb91-bfa34d38d872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer=SnowballStemmer('english')\n",
        "for word in words:\n",
        "  print(word,'|',snowball_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi1ZwMz9puCm",
        "outputId": "7bf659b1-67e6-48a8-ce8e-85e84cc591f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | ate\n",
            "Adjustable | adjust\n",
            "rafting | raft\n",
            "ability | abil\n",
            "meeting | meet\n",
            "history | histori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between PorterStemmer and SnowballStemmer\n",
        "print('PorterStemmer--> '+ stemming.stem('Generous')+',\\t'+ stemming.stem('Fairly')+',\\t'+stemming.stem('sportingly'))\n",
        "print('SnowballStemmer--> '+ snowball_stemmer.stem('Generous')+',\\t'+ snowball_stemmer.stem('Fairly')+',\\t'+snowball_stemmer.stem('sportingly'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrODFpSPqSa9",
        "outputId": "d96426a9-7860-446c-a495-abb33b74b9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer--> gener,\tfairli,\tsportingli\n",
            "SnowballStemmer--> generous,\tfair,\tsport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Lemmatization"
      ],
      "metadata": {
        "id": "MHsrP4YIudOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet Lammatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpDZ-HBVqpc_",
        "outputId": "cbffff26-49de-4564-df5e-b9bdc80db02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('going')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nLHjQ234u2BR",
        "outputId": "77826388-3211-4f6a-b755-3ddd929095e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('going',pos='n'))    #pos(parts of speech) tag--> noun\n",
        "print(lemmatizer.lemmatize('going',pos='v'))    #v-->verb\n",
        "print(lemmatizer.lemmatize('going',pos='a'))    #a-->adjective\n",
        "print(lemmatizer.lemmatize('going',pos='r'))    #r-->adverb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMF0OmBxvIfp",
        "outputId": "9eae69fc-f248-4690-acc2-33aa8198f318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going\n",
            "go\n",
            "going\n",
            "going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,'|',lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbfxNNMWxBVF",
        "outputId": "05eacc68-59f1-4a86-b1f5-ad8427fde366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating | Eating\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | eat\n",
            "Adjustable | Adjustable\n",
            "rafting | raft\n",
            "ability | ability\n",
            "meeting | meet\n",
            "history | history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Stopword"
      ],
      "metadata": {
        "id": "JFnxlwsq5XQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\" In today’s digital world, the amount of textual data generated every second is staggering.\n",
        "From social media updates and customer reviews to news articles and research papers, unstructured text is everywhere.\n",
        "Natural Language Processing, commonly known as NLP, plays a crucial role in transforming this raw data into meaningful insights.\n",
        "Businesses use NLP to analyze customer sentiment, automate support services, and detect trends in market behavior.\n",
        "For example, sentiment analysis helps companies understand how their products are perceived by users,\n",
        "while chatbots powered by NLP provide instant customer assistance without human intervention.\n",
        "Despite its success, NLP still faces challenges when it comes to context understanding, irony, cultural nuances, and multilingual support.\n",
        "With the rapid advancements in machine learning and deep learning, however, NLP is constantly evolving, making machines more capable of truly understanding human language.\n",
        "Researchers are working on large language models, better datasets, and more accurate algorithms to bridge the gap between human and machine communication. History. Generous.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WxX8J6SsxK1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLNfW1a99TzU",
        "outputId": "5148a990-a738-4243-882a-299d1901641a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('english')\n",
        "stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn7LZY-X9jcS",
        "outputId": "8912865f-5087-4464-b2fe-ad13154c54b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "temp=sentences"
      ],
      "metadata": {
        "id": "x_8pBsHl9rS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "# Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords)]\n",
        "  temp[i]=' '.join(words)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruYSss6-u3-",
        "outputId": "3033f19c-6943-489a-b91b-a19cae2fcea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in today ’ digit world , amount textual data gener everi second stagger .',\n",
              " 'from social media updat custom review news articl research paper , unstructur text everywher .',\n",
              " 'natur languag process , commonli known nlp , play crucial role transform raw data meaning insight .',\n",
              " 'busi use nlp analyz custom sentiment , autom support servic , detect trend market behavior .',\n",
              " 'for exampl , sentiment analysi help compani understand product perceiv user , chatbot power nlp provid instant custom assist without human intervent .',\n",
              " 'despit success , nlp still face challeng come context understand , ironi , cultur nuanc , multilingu support .',\n",
              " 'with rapid advanc machin learn deep learn , howev , nlp constantli evolv , make machin capabl truli understand human languag .',\n",
              " 'research work larg languag model , better dataset , accur algorithm bridg gap human machin commun .',\n",
              " 'histori .',\n",
              " 'gener .']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')\n",
        "# Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords)]\n",
        "  temp[i]=' '.join(words)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jaeeXNS_5M8",
        "outputId": "d68b2a7f-eb66-4c6b-f7b4-e4cdfcd62181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in today ’ digit world , amount textual data generat everi second stagger .',\n",
              " 'from social media updat custom review news articl research paper , unstructur text everywher .',\n",
              " 'natur languag process , common know nlp , play crucial role transform raw data meaning insight .',\n",
              " 'busi use nlp analyz custom sentiment , autom support servic , detect trend market behavior .',\n",
              " 'for exampl , sentiment analysi help compani understand product perceiv user , chatbot power nlp provid instant custom assist without human intervent .',\n",
              " 'despit success , nlp still face challeng come context understand , ironi , cultur nuanc , multilingu support .',\n",
              " 'with rapid advanc machin learn deep learn , howev , nlp constant evolv , make machin capabl truli understand human languag .',\n",
              " 'research work larg languag model , better dataset , accur algorithm bridg gap human machin communic .',\n",
              " 'histori .',\n",
              " 'generous .']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "# Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)]\n",
        "  temp[i]=' '.join(words)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmxiuxK4BoXR",
        "outputId": "f6b9c364-73d4-4b1a-b6be-6fb21ca30ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In today ’ digital world , amount textual data generated every second staggering .',\n",
              " 'From social medium update customer review news article research paper , unstructured text everywhere .',\n",
              " 'Natural Language Processing , commonly known NLP , play crucial role transforming raw data meaningful insight .',\n",
              " 'Businesses use NLP analyze customer sentiment , automate support service , detect trend market behavior .',\n",
              " 'For example , sentiment analysis help company understand product perceived user , chatbots powered NLP provide instant customer assistance without human intervention .',\n",
              " 'Despite success , NLP still face challenge come context understanding , irony , cultural nuance , multilingual support .',\n",
              " 'With rapid advancement machine learning deep learning , however , NLP constantly evolving , making machine capable truly understanding human language .',\n",
              " 'Researchers working large language model , better datasets , accurate algorithm bridge gap human machine communication .',\n",
              " 'History .',\n",
              " 'Generous .']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "# Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(word.lower()) for word in words if word not in set(stopwords)]    #word.lower()\n",
        "  temp[i]=' '.join(words)\n",
        "temp"
      ],
      "metadata": {
        "id": "QqzHBXGsCUcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ac88eb-94a2-4c95-9d5b-6f334218fbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in today ’ digital world , amount textual data generated every second staggering .',\n",
              " 'from social medium update customer review news article research paper , unstructured text everywhere .',\n",
              " 'natural language processing , commonly known nlp , play crucial role transforming raw data meaningful insight .',\n",
              " 'business use nlp analyze customer sentiment , automate support service , detect trend market behavior .',\n",
              " 'for example , sentiment analysis help company understand product perceived user , chatbots powered nlp provide instant customer assistance without human intervention .',\n",
              " 'despite success , nlp still face challenge come context understanding , irony , cultural nuance , multilingual support .',\n",
              " 'with rapid advancement machine learning deep learning , however , nlp constantly evolving , making machine capable truly understanding human language .',\n",
              " 'researcher working large language model , better datasets , accurate algorithm bridge gap human machine communication .',\n",
              " 'history .',\n",
              " 'generous .']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "# Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(word, pos='v') for word in words if word not in set(stopwords)]    #word.lower()\n",
        "  temp[i]=' '.join(words)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7lMSzedbH_",
        "outputId": "0a122acb-f496-4a0a-bc7a-fe920fa13a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In today ’ digital world , amount textual data generate every second stagger .',\n",
              " 'From social media update customer review news article research paper , unstructured text everywhere .',\n",
              " 'Natural Language Processing , commonly know NLP , play crucial role transform raw data meaningful insights .',\n",
              " 'Businesses use NLP analyze customer sentiment , automate support service , detect trend market behavior .',\n",
              " 'For example , sentiment analysis help company understand products perceive users , chatbots power NLP provide instant customer assistance without human intervention .',\n",
              " 'Despite success , NLP still face challenge come context understand , irony , cultural nuances , multilingual support .',\n",
              " 'With rapid advancements machine learn deep learn , however , NLP constantly evolve , make machine capable truly understand human language .',\n",
              " 'Researchers work large language model , better datasets , accurate algorithms bridge gap human machine communication .',\n",
              " 'History .',\n",
              " 'Generous .']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Parts of Speech\n",
        " Penn Treebank POS Tags (widely used in NLM/NLP):\n",
        "\n",
        "CC – Coordinating conjunction (e.g., and, but, or)\n",
        "\n",
        "CD – Cardinal number (e.g., one, two, 3.14)\n",
        "\n",
        "DT – Determiner (e.g., the, a, an)\n",
        "\n",
        "EX – Existential “there” (e.g., there is)\n",
        "\n",
        "FW – Foreign word\n",
        "\n",
        "IN – Preposition or subordinating conjunction (e.g., in, of, although)\n",
        "\n",
        "JJ – Adjective (e.g., red, quick)\n",
        "\n",
        "JJR – Comparative adjective (e.g., bigger, faster)\n",
        "\n",
        "JJS – Superlative adjective (e.g., biggest, best)\n",
        "\n",
        "LS – List item marker (e.g., 1), A))\n",
        "\n",
        "MD – Modal verb (e.g., can, should, must)\n",
        "\n",
        "NN – Noun, singular (e.g., cat, truth)\n",
        "\n",
        "NNS – Noun, plural (e.g., cats, cars)\n",
        "\n",
        "NNP – Proper noun, singular (e.g., India, John)\n",
        "\n",
        "NNPS – Proper noun, plural (e.g., Americans)\n",
        "\n",
        "PDT – Predeterminer (e.g., all, both in \"both the boys\")\n",
        "\n",
        "POS – Possessive ending (e.g., ’s)\n",
        "\n",
        "PRP – Personal pronoun (e.g., I, you, he)\n",
        "\n",
        "PRP$ – Possessive pronoun (e.g., my, his, their)\n",
        "\n",
        "RB – Adverb (e.g., quickly, very)\n",
        "\n",
        "RBR – Comparative adverb (e.g., faster)\n",
        "\n",
        "RBS – Superlative adverb (e.g., fastest)\n",
        "\n",
        "RP – Particle (e.g., up, off in \"give up\")\n",
        "\n",
        "SYM – Symbol (e.g., $, %, =)\n",
        "\n",
        "TO – The word “to” as in \"to go\"\n",
        "\n",
        "UH – Interjection (e.g., oh, wow, uh)\n",
        "\n",
        "VB – Verb, base form (e.g., go, eat)\n",
        "\n",
        "VBD – Verb, past tense (e.g., went, ate)\n",
        "\n",
        "VBG – Verb, gerund/present participle (e.g., going, eating)\n",
        "\n",
        "VBN – Verb, past participle (e.g., gone, eaten)\n",
        "\n",
        "VBP – Verb, non-3rd person singular present (e.g., go, eat)\n",
        "\n",
        "VBZ – Verb, 3rd person singular present (e.g., goes, eats)\n",
        "\n",
        "WDT – Wh-determiner (e.g., which, that)\n",
        "\n",
        "WP – Wh-pronoun (e.g., who, what)\n",
        "\n",
        "WP$ – Possessive wh-pronoun (e.g., whose)\n",
        "\n",
        "WRB – Wh-adverb (e.g., when, where, why)\n"
      ],
      "metadata": {
        "id": "krOIhyFbNuAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "QleSA73LhlWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rby5-3Y8PdxR",
        "outputId": "d8549227-a694-4fb2-82e8-e6224e36cdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "  pos_tag=nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGlcDC5MP5az",
        "outputId": "ee116d14-d710-48aa-81d4-e54d6ac02eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
            "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
            "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag('Today is a regular day'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SFCzP-2Q2dW",
        "outputId": "e4151e26-69d8-43ca-a11d-b48d6c9c4de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('regular', 'JJ'), ('day', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Named Entity Recognization"
      ],
      "metadata": {
        "id": "_PVkdab3SuVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
      ],
      "metadata": {
        "id": "J9Gj1BZvRxwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "words=nltk.word_tokenize(sentence)\n",
        "pos_tags=nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "ssgRmQTISYkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjidSgxFWQ5j",
        "outputId": "3aef3ee4-0ae7-490e-a211-469a66a48471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.ne_chunk(pos_tags).draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "DdjmN09UWcJV",
        "outputId": "abcc115e-098b-434e-a2f9-6ed2222a7a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-53afac5e5473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-hdBXrPXd61",
        "outputId": "7874bd8a-44c1-42bf-d193-bb91c66362df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.draw.tree import TreeView\n",
        "import svgling\n",
        "\n",
        "# After creating the chunks\n",
        "chunks = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# using svgling to display an SVG representation of the tree\n",
        "svg = svgling.draw_tree(chunks)\n",
        "display(svg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "ElqskxpMXkj8",
        "outputId": "774a883d-6b03-4e15-a6c7-767243804e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TreeLayout(Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')]))"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1280.0,168.0\" width=\"1280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.125%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.5625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.375%\" x=\"3.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"12.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.0625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"20%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"23.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"27.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"30%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"33.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.625%\" x=\"36.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"46.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"48.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.9375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.625%\" x=\"53.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.9375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.125%\" x=\"58.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"66.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"69.375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"75.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"87.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.0625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"90.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"98.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.0625%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Different types of Encoding\n",
        "\n",
        "\n",
        "*   One Hot Encoding\n",
        "*   Bag of words (BOW)\n",
        "*   TF-IDF\n",
        "*   Word2vec\n",
        "*   AvgWord2vec\n",
        "\n"
      ],
      "metadata": {
        "id": "iccriKMtZYmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Word2Vec"
      ],
      "metadata": {
        "id": "xKPtQMXo_oRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "O5vJrNHBXoW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46fea02-278d-47a1-8c36-674c9c16d9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m859.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "6CVocZJh_1BB",
        "outputId": "a5ba3518-22ed-49e2-8af1-0e9e54e6dc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dccafb1dab4f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocess_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy\n",
        "!pip install scipy\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "ER9YapfqAHMg",
        "outputId": "48cfa6e7-543c-45cf-cfd1-730de7f4c526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.2.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d4a3a331d73e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scipy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocess_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m                                  \"numpy 2.0 onwards\", name=None)\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"strings\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/strings/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/strings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from numpy._core.umath import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0misalpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0misdigit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy\n",
        "!pip uninstall scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA6JtUrXA1wF",
        "outputId": "4f0cf29d-e8db-4bd0-ed13-ac4fbad2afe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.2.4\n",
            "Uninstalling numpy-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-2.2.4\n",
            "Found existing installation: scipy 1.13.1\n",
            "Uninstalling scipy-1.13.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy-1.13.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libopenblasp-r0-01191904.3.27.so\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25.2\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0H46aFqkBIOC",
        "outputId": "6084f8ef-aa72-4fe2-c7c9-dace940591bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "pymc 5.21.2 requires scipy>=1.4.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "shap 0.47.1 requires scipy, which is not installed.\n",
            "osqp 1.0.3 requires scipy>=0.13.2, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "albumentations 2.0.5 requires scipy>=1.10.0, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "sentence-transformers 3.4.1 requires scipy, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "scikit-learn 1.6.1 requires scipy>=1.6.0, which is not installed.\n",
            "pytensor 2.30.2 requires scipy<2,>=1, which is not installed.\n",
            "cvxpy 1.6.4 requires scipy>=1.11.0, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "mizani 0.13.2 requires scipy>=1.8.0, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "10bd3afe025440ac9cc78e3632e66f43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.25.2)\n",
            "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "metadata": {
        "id": "BQBU-FE2BcfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv=api.load('word2vec-google-news-300')\n",
        "vec_king=wv['king']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyPs5kUuBq_W",
        "outputId": "f1b898f6-eef1-4b1b-9a35-b3a5375d8622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_king"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIOzu-fLCJPn",
        "outputId": "313b5dfe-bd37-408e-a48e-5c63fe42b2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
              "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
              "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
              "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
              "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
              "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
              "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
              "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
              "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
              "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
              "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
              "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
              "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
              "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
              "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
              "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
              "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
              "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
              "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
              "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
              "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
              "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
              "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
              "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
              "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
              "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
              "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
              "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
              "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
              "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
              "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
              "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
              "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
              "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
              "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
              "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
              "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
              "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
              "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
              "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
              "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
              "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
              "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
              "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
              "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
              "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
              "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
              "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
              "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
              "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
              "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
              "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
              "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
              "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
              "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
              "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
              "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
              "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
              "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
              "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
              "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
              "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
              "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
              "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
              "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
              "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
              "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
              "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
              "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
              "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
              "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
              "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
              "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
              "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
              "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_king.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5eQuJjmEjbf",
        "outputId": "c8da7923-afdc-4b07-eb5a-6540b1da5c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv['cricket']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdtRvwvqGRdt",
        "outputId": "d484608c-6c92-4fef-d219-0b235b86b585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.67187500e-01, -1.21582031e-01,  2.85156250e-01,  8.15429688e-02,\n",
              "        3.19824219e-02, -3.19824219e-02,  1.34765625e-01, -2.73437500e-01,\n",
              "        9.46044922e-03, -1.07421875e-01,  2.48046875e-01, -6.05468750e-01,\n",
              "        5.02929688e-02,  2.98828125e-01,  9.57031250e-02,  1.39648438e-01,\n",
              "       -5.41992188e-02,  2.91015625e-01,  2.85156250e-01,  1.51367188e-01,\n",
              "       -2.89062500e-01, -3.46679688e-02,  1.81884766e-02, -3.92578125e-01,\n",
              "        2.46093750e-01,  2.51953125e-01, -9.86328125e-02,  3.22265625e-01,\n",
              "        4.49218750e-01, -1.36718750e-01, -2.34375000e-01,  4.12597656e-02,\n",
              "       -2.15820312e-01,  1.69921875e-01,  2.56347656e-02,  1.50146484e-02,\n",
              "       -3.75976562e-02,  6.95800781e-03,  4.00390625e-01,  2.09960938e-01,\n",
              "        1.17675781e-01, -4.19921875e-02,  2.34375000e-01,  2.03125000e-01,\n",
              "       -1.86523438e-01, -2.46093750e-01,  3.12500000e-01, -2.59765625e-01,\n",
              "       -1.06933594e-01,  1.04003906e-01, -1.79687500e-01,  5.71289062e-02,\n",
              "       -7.41577148e-03, -5.59082031e-02,  7.61718750e-02, -4.14062500e-01,\n",
              "       -3.65234375e-01, -3.35937500e-01, -1.54296875e-01, -2.39257812e-01,\n",
              "       -3.73046875e-01,  2.27355957e-03, -3.51562500e-01,  8.64257812e-02,\n",
              "        1.26953125e-01,  2.21679688e-01, -9.86328125e-02,  1.08886719e-01,\n",
              "        3.65234375e-01, -5.66406250e-02,  5.66406250e-02, -1.09375000e-01,\n",
              "       -1.66992188e-01, -4.54101562e-02, -2.00195312e-01, -1.22558594e-01,\n",
              "        1.31835938e-01, -1.31835938e-01,  1.03027344e-01, -3.41796875e-01,\n",
              "       -1.57226562e-01,  2.04101562e-01,  4.39453125e-02,  2.44140625e-01,\n",
              "       -3.19824219e-02,  3.20312500e-01, -4.41894531e-02,  1.08398438e-01,\n",
              "       -4.98046875e-02, -9.52148438e-03,  2.46093750e-01, -5.59082031e-02,\n",
              "        4.07714844e-02, -1.78222656e-02, -2.95410156e-02,  1.65039062e-01,\n",
              "        5.03906250e-01, -2.81250000e-01,  9.81445312e-02,  1.80664062e-02,\n",
              "       -1.83593750e-01,  2.53906250e-01,  2.25585938e-01,  1.63574219e-02,\n",
              "        1.81640625e-01,  1.38671875e-01,  3.33984375e-01,  1.39648438e-01,\n",
              "        1.45874023e-02, -2.89306641e-02, -8.39843750e-02,  1.50390625e-01,\n",
              "        1.67968750e-01,  2.28515625e-01,  3.59375000e-01,  1.22558594e-01,\n",
              "       -3.28125000e-01, -1.56250000e-01,  2.77343750e-01,  1.77001953e-02,\n",
              "       -1.46484375e-01, -4.51660156e-03, -4.46777344e-02,  1.75781250e-01,\n",
              "       -3.75000000e-01,  1.16699219e-01, -1.39648438e-01,  2.55859375e-01,\n",
              "       -1.96289062e-01, -2.57568359e-02, -5.41992188e-02, -2.51464844e-02,\n",
              "       -1.93359375e-01, -3.17382812e-02, -8.74023438e-02, -1.32812500e-01,\n",
              "       -2.12402344e-02,  4.33593750e-01, -5.20019531e-02,  3.46679688e-02,\n",
              "        8.00781250e-02,  3.41796875e-02,  1.99218750e-01, -2.39257812e-02,\n",
              "       -2.37304688e-01,  1.93359375e-01,  7.32421875e-02, -2.87109375e-01,\n",
              "        1.25000000e-01,  8.44726562e-02,  1.30859375e-01, -2.19726562e-01,\n",
              "       -1.61132812e-01, -2.63671875e-01, -5.46875000e-01, -2.96875000e-01,\n",
              "        3.44238281e-02, -2.87109375e-01, -1.93359375e-01, -1.61132812e-01,\n",
              "       -3.84765625e-01, -2.14843750e-01, -6.22558594e-03, -1.27929688e-01,\n",
              "       -1.00097656e-01, -6.21093750e-01,  3.78906250e-01, -4.58984375e-01,\n",
              "        1.44531250e-01, -9.13085938e-02, -3.08593750e-01,  2.23632812e-01,\n",
              "        7.86132812e-02, -2.16796875e-01,  8.78906250e-02, -1.66992188e-01,\n",
              "        1.14746094e-02, -2.53906250e-01, -6.25000000e-02,  6.04248047e-03,\n",
              "        1.56250000e-01,  4.37500000e-01, -2.23632812e-01, -2.32421875e-01,\n",
              "        2.75390625e-01,  2.39257812e-01,  4.49218750e-02, -7.51953125e-02,\n",
              "        5.74218750e-01, -2.61230469e-02, -1.21582031e-01,  2.44140625e-01,\n",
              "       -3.37890625e-01,  8.59375000e-02, -7.71484375e-02,  4.85839844e-02,\n",
              "        1.43554688e-01,  4.25781250e-01, -4.29687500e-02, -1.08398438e-01,\n",
              "        1.19628906e-01, -1.91406250e-01, -2.12890625e-01, -2.87109375e-01,\n",
              "       -1.14746094e-01, -2.04101562e-01, -2.06298828e-02, -2.53906250e-01,\n",
              "        8.25195312e-02, -3.97949219e-02, -1.57226562e-01,  1.34765625e-01,\n",
              "        2.08007812e-01, -1.78710938e-01, -2.00195312e-02, -8.34960938e-02,\n",
              "       -1.20605469e-01,  4.29687500e-02, -1.94335938e-01, -1.32812500e-01,\n",
              "       -2.17285156e-02, -2.35351562e-01, -3.63281250e-01,  1.51367188e-01,\n",
              "        9.32617188e-02,  1.63085938e-01,  1.02050781e-01, -4.27734375e-01,\n",
              "        2.83203125e-01,  2.74658203e-04, -3.20312500e-01,  1.68457031e-02,\n",
              "        4.06250000e-01, -5.24902344e-02,  7.91015625e-02, -1.41601562e-01,\n",
              "        5.27343750e-01, -1.26953125e-01,  4.74609375e-01, -6.64062500e-02,\n",
              "        3.41796875e-01, -1.78710938e-01,  3.69140625e-01, -2.05078125e-01,\n",
              "        5.82885742e-03, -1.84570312e-01, -8.88671875e-02, -1.81640625e-01,\n",
              "       -4.80957031e-02,  4.39453125e-01,  2.12890625e-01, -3.07617188e-02,\n",
              "        9.32617188e-02,  2.40234375e-01,  2.39257812e-01,  2.51953125e-01,\n",
              "       -1.98974609e-02,  1.24511719e-01, -4.73632812e-02, -2.13623047e-02,\n",
              "        3.12500000e-02,  3.05175781e-02,  2.79296875e-01,  9.08203125e-02,\n",
              "       -2.02148438e-01, -2.19726562e-02, -2.63671875e-01,  8.78906250e-02,\n",
              "       -1.07421875e-01, -2.49023438e-01, -1.22070312e-02,  1.73828125e-01,\n",
              "       -9.91210938e-02,  7.27539062e-02,  2.59765625e-01, -4.60937500e-01,\n",
              "        3.59375000e-01, -2.25585938e-01,  1.87988281e-02, -2.19726562e-01,\n",
              "       -2.08984375e-01, -1.51367188e-01,  8.64257812e-02,  1.11694336e-02,\n",
              "        6.93359375e-02, -2.99072266e-02,  1.43554688e-01,  1.89453125e-01,\n",
              "       -1.32812500e-01,  4.72656250e-01, -1.40625000e-01, -2.52685547e-02,\n",
              "        1.91406250e-01, -2.63671875e-01, -1.39648438e-01,  1.09375000e-01,\n",
              "        1.97753906e-02,  2.49023438e-01, -1.42578125e-01,  4.15039062e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar('cricket')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPfE7mg6GUQ4",
        "outputId": "69eb273a-65eb-4989-e93f-a9c8ffd80272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cricketing', 0.8372225761413574),\n",
              " ('cricketers', 0.8165745735168457),\n",
              " ('Test_cricket', 0.8094819188117981),\n",
              " ('Twenty##_cricket', 0.8068488240242004),\n",
              " ('Twenty##', 0.7624265551567078),\n",
              " ('Cricket', 0.75413978099823),\n",
              " ('cricketer', 0.7372578382492065),\n",
              " ('twenty##', 0.7316356897354126),\n",
              " ('T##_cricket', 0.7304614186286926),\n",
              " ('West_Indies_cricket', 0.6987985968589783)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity('man','woman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stzZsSA-GbT2",
        "outputId": "4e3bba5b-715b-4912-f41c-001aca179e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76640123"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec=wv['king']-wv['man']+wv['woman']"
      ],
      "metadata": {
        "id": "XznWQ_yqGoco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar([vec])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nuUnXViG1PC",
        "outputId": "418f3ecf-3ef2-4810-db0b-bb57ccb1f79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.645466148853302),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676352500916),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376775860786438),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Scratch word2vec"
      ],
      "metadata": {
        "id": "eBXsmghSeF4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# 1. Sample corpus (list of tokenized sentences)\n",
        "corpus = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog ran in the park\",\n",
        "    \"cats love to play with dogs\"\n",
        "]\n",
        "\n",
        "# 2. Preprocess the corpus (tokenize and lowercase)\n",
        "sentences = [simple_preprocess(sentence) for sentence in corpus]\n",
        "# Result: [['the', 'cat', 'sat', 'on', 'the', 'mat'], ...]\n",
        "\n",
        "# 3. Train the Word2Vec model (Skip-Gram)\n",
        "model = Word2Vec(\n",
        "    sentences=sentences,        # Input: list of tokenized sentences\n",
        "    vector_size=100,           # Embedding dimension\n",
        "    window=5,                  # Context window size\n",
        "    sg=1,                      # 1 for Skip-Gram, 0 for CBOW\n",
        "    min_count=1,               # Min word frequency to include\n",
        "    workers=4,                 # Number of CPU cores\n",
        "    negative=5,                # Number of negative samples\n",
        "    epochs=10                  # Training iterations\n",
        ")\n",
        "\n",
        "# 4. Access results\n",
        "# Get embedding for a word\n",
        "# print(\"Embedding for 'cat':\", model.wv['cat'])\n",
        "\n",
        "# Find similar words\n",
        "print(\"Words similar to 'cat':\", model.wv.most_similar('cat', topn=5))\n",
        "\n",
        "# Compute similarity between two words\n",
        "print(\"Similarity between 'cat' and 'dog':\", model.wv.similarity('cat', 'dog'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhh5rKFqG78q",
        "outputId": "bcef71a5-f601-4e56-fdfb-ab050a4fc086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'cat': [('play', 0.17826786637306213), ('dogs', 0.16072483360767365), ('park', 0.10560770332813263), ('on', 0.09215974807739258), ('sat', 0.027008360251784325)]\n",
            "Similarity between 'cat' and 'dog': -0.046497855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. BPE(byte pair embedding) + word2vec"
      ],
      "metadata": {
        "id": "UaCfWcthbfLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =\"\"\"In today’s digital world, the amount of textual data generated every second is staggering.\n",
        "From social media updates and customer reviews to news articles and research papers, unstructured text is everywhere.\n",
        "Natural Language Processing, commonly known as NLP, plays a crucial role in transforming this raw data into meaningful insights.\n",
        "Businesses use NLP to analyze customer sentiment, automate support services, and detect trends in market behavior.\n",
        "For example, sentiment analysis helps companies understand how their products are perceived by users,\n",
        "while chatbots powered by NLP provide instant customer assistance without human intervention.\n",
        "Despite its success, NLP still faces challenges when it comes to context understanding, irony, cultural nuances, and multilingual support.\n",
        "With the rapid advancements in machine learning and deep learning, however, NLP is constantly evolving, making machines more capable of truly understanding human language.\n",
        "Researchers are working on large language models, better datasets, and more accurate algorithms to bridge the gap between human and machine communication. History. Generous.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sKToNmfCerQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "words=nltk.word_tokenize(corpus)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS8EHiWTrWhp",
        "outputId": "0ef94626-6bd4-4634-b170-a0c9c593eea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'today',\n",
              " '’',\n",
              " 's',\n",
              " 'digital',\n",
              " 'world',\n",
              " ',',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'textual',\n",
              " 'data',\n",
              " 'generated',\n",
              " 'every',\n",
              " 'second',\n",
              " 'is',\n",
              " 'staggering',\n",
              " '.',\n",
              " 'From',\n",
              " 'social',\n",
              " 'media',\n",
              " 'updates',\n",
              " 'and',\n",
              " 'customer',\n",
              " 'reviews',\n",
              " 'to',\n",
              " 'news',\n",
              " 'articles',\n",
              " 'and',\n",
              " 'research',\n",
              " 'papers',\n",
              " ',',\n",
              " 'unstructured',\n",
              " 'text',\n",
              " 'is',\n",
              " 'everywhere',\n",
              " '.',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " ',',\n",
              " 'commonly',\n",
              " 'known',\n",
              " 'as',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'plays',\n",
              " 'a',\n",
              " 'crucial',\n",
              " 'role',\n",
              " 'in',\n",
              " 'transforming',\n",
              " 'this',\n",
              " 'raw',\n",
              " 'data',\n",
              " 'into',\n",
              " 'meaningful',\n",
              " 'insights',\n",
              " '.',\n",
              " 'Businesses',\n",
              " 'use',\n",
              " 'NLP',\n",
              " 'to',\n",
              " 'analyze',\n",
              " 'customer',\n",
              " 'sentiment',\n",
              " ',',\n",
              " 'automate',\n",
              " 'support',\n",
              " 'services',\n",
              " ',',\n",
              " 'and',\n",
              " 'detect',\n",
              " 'trends',\n",
              " 'in',\n",
              " 'market',\n",
              " 'behavior',\n",
              " '.',\n",
              " 'For',\n",
              " 'example',\n",
              " ',',\n",
              " 'sentiment',\n",
              " 'analysis',\n",
              " 'helps',\n",
              " 'companies',\n",
              " 'understand',\n",
              " 'how',\n",
              " 'their',\n",
              " 'products',\n",
              " 'are',\n",
              " 'perceived',\n",
              " 'by',\n",
              " 'users',\n",
              " ',',\n",
              " 'while',\n",
              " 'chatbots',\n",
              " 'powered',\n",
              " 'by',\n",
              " 'NLP',\n",
              " 'provide',\n",
              " 'instant',\n",
              " 'customer',\n",
              " 'assistance',\n",
              " 'without',\n",
              " 'human',\n",
              " 'intervention',\n",
              " '.',\n",
              " 'Despite',\n",
              " 'its',\n",
              " 'success',\n",
              " ',',\n",
              " 'NLP',\n",
              " 'still',\n",
              " 'faces',\n",
              " 'challenges',\n",
              " 'when',\n",
              " 'it',\n",
              " 'comes',\n",
              " 'to',\n",
              " 'context',\n",
              " 'understanding',\n",
              " ',',\n",
              " 'irony',\n",
              " ',',\n",
              " 'cultural',\n",
              " 'nuances',\n",
              " ',',\n",
              " 'and',\n",
              " 'multilingual',\n",
              " 'support',\n",
              " '.',\n",
              " 'With',\n",
              " 'the',\n",
              " 'rapid',\n",
              " 'advancements',\n",
              " 'in',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'and',\n",
              " 'deep',\n",
              " 'learning',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'NLP',\n",
              " 'is',\n",
              " 'constantly',\n",
              " 'evolving',\n",
              " ',',\n",
              " 'making',\n",
              " 'machines',\n",
              " 'more',\n",
              " 'capable',\n",
              " 'of',\n",
              " 'truly',\n",
              " 'understanding',\n",
              " 'human',\n",
              " 'language',\n",
              " '.',\n",
              " 'Researchers',\n",
              " 'are',\n",
              " 'working',\n",
              " 'on',\n",
              " 'large',\n",
              " 'language',\n",
              " 'models',\n",
              " ',',\n",
              " 'better',\n",
              " 'datasets',\n",
              " ',',\n",
              " 'and',\n",
              " 'more',\n",
              " 'accurate',\n",
              " 'algorithms',\n",
              " 'to',\n",
              " 'bridge',\n",
              " 'the',\n",
              " 'gap',\n",
              " 'between',\n",
              " 'human',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'communication',\n",
              " '.',\n",
              " 'History',\n",
              " '.',\n",
              " 'Generous',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=corpus.split()\n",
        "words=[word+'_' for word in a]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dySyUWrGsAxG",
        "outputId": "90ff3df0-b63a-4eeb-b77b-6c754302e1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In_', 'today’s_', 'digital_', 'world,_', 'the_', 'amount_', 'of_', 'textual_', 'data_', 'generated_', 'every_', 'second_', 'is_', 'staggering._', 'From_', 'social_', 'media_', 'updates_', 'and_', 'customer_', 'reviews_', 'to_', 'news_', 'articles_', 'and_', 'research_', 'papers,_', 'unstructured_', 'text_', 'is_', 'everywhere._', 'Natural_', 'Language_', 'Processing,_', 'commonly_', 'known_', 'as_', 'NLP,_', 'plays_', 'a_', 'crucial_', 'role_', 'in_', 'transforming_', 'this_', 'raw_', 'data_', 'into_', 'meaningful_', 'insights._', 'Businesses_', 'use_', 'NLP_', 'to_', 'analyze_', 'customer_', 'sentiment,_', 'automate_', 'support_', 'services,_', 'and_', 'detect_', 'trends_', 'in_', 'market_', 'behavior._', 'For_', 'example,_', 'sentiment_', 'analysis_', 'helps_', 'companies_', 'understand_', 'how_', 'their_', 'products_', 'are_', 'perceived_', 'by_', 'users,_', 'while_', 'chatbots_', 'powered_', 'by_', 'NLP_', 'provide_', 'instant_', 'customer_', 'assistance_', 'without_', 'human_', 'intervention._', 'Despite_', 'its_', 'success,_', 'NLP_', 'still_', 'faces_', 'challenges_', 'when_', 'it_', 'comes_', 'to_', 'context_', 'understanding,_', 'irony,_', 'cultural_', 'nuances,_', 'and_', 'multilingual_', 'support._', 'With_', 'the_', 'rapid_', 'advancements_', 'in_', 'machine_', 'learning_', 'and_', 'deep_', 'learning,_', 'however,_', 'NLP_', 'is_', 'constantly_', 'evolving,_', 'making_', 'machines_', 'more_', 'capable_', 'of_', 'truly_', 'understanding_', 'human_', 'language._', 'Researchers_', 'are_', 'working_', 'on_', 'large_', 'language_', 'models,_', 'better_', 'datasets,_', 'and_', 'more_', 'accurate_', 'algorithms_', 'to_', 'bridge_', 'the_', 'gap_', 'between_', 'human_', 'and_', 'machine_', 'communication._', 'History._', 'Generous._']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate frequency1\n",
        "from collections import Counter\n",
        "\n",
        "freqs = Counter(words)\n",
        "print(\"Word frequencies:\", freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCvWFSs9tCV0",
        "outputId": "b9fe9b23-06c6-417d-fe15-f15edde10243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequencies: Counter({'and_': 7, 'to_': 4, 'NLP_': 4, 'the_': 3, 'is_': 3, 'customer_': 3, 'in_': 3, 'human_': 3, 'of_': 2, 'data_': 2, 'are_': 2, 'by_': 2, 'machine_': 2, 'more_': 2, 'In_': 1, 'today’s_': 1, 'digital_': 1, 'world,_': 1, 'amount_': 1, 'textual_': 1, 'generated_': 1, 'every_': 1, 'second_': 1, 'staggering._': 1, 'From_': 1, 'social_': 1, 'media_': 1, 'updates_': 1, 'reviews_': 1, 'news_': 1, 'articles_': 1, 'research_': 1, 'papers,_': 1, 'unstructured_': 1, 'text_': 1, 'everywhere._': 1, 'Natural_': 1, 'Language_': 1, 'Processing,_': 1, 'commonly_': 1, 'known_': 1, 'as_': 1, 'NLP,_': 1, 'plays_': 1, 'a_': 1, 'crucial_': 1, 'role_': 1, 'transforming_': 1, 'this_': 1, 'raw_': 1, 'into_': 1, 'meaningful_': 1, 'insights._': 1, 'Businesses_': 1, 'use_': 1, 'analyze_': 1, 'sentiment,_': 1, 'automate_': 1, 'support_': 1, 'services,_': 1, 'detect_': 1, 'trends_': 1, 'market_': 1, 'behavior._': 1, 'For_': 1, 'example,_': 1, 'sentiment_': 1, 'analysis_': 1, 'helps_': 1, 'companies_': 1, 'understand_': 1, 'how_': 1, 'their_': 1, 'products_': 1, 'perceived_': 1, 'users,_': 1, 'while_': 1, 'chatbots_': 1, 'powered_': 1, 'provide_': 1, 'instant_': 1, 'assistance_': 1, 'without_': 1, 'intervention._': 1, 'Despite_': 1, 'its_': 1, 'success,_': 1, 'still_': 1, 'faces_': 1, 'challenges_': 1, 'when_': 1, 'it_': 1, 'comes_': 1, 'context_': 1, 'understanding,_': 1, 'irony,_': 1, 'cultural_': 1, 'nuances,_': 1, 'multilingual_': 1, 'support._': 1, 'With_': 1, 'rapid_': 1, 'advancements_': 1, 'learning_': 1, 'deep_': 1, 'learning,_': 1, 'however,_': 1, 'constantly_': 1, 'evolving,_': 1, 'making_': 1, 'machines_': 1, 'capable_': 1, 'truly_': 1, 'understanding_': 1, 'language._': 1, 'Researchers_': 1, 'working_': 1, 'on_': 1, 'large_': 1, 'language_': 1, 'models,_': 1, 'better_': 1, 'datasets,_': 1, 'accurate_': 1, 'algorithms_': 1, 'bridge_': 1, 'gap_': 1, 'between_': 1, 'communication._': 1, 'History._': 1, 'Generous._': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate frequency2\n",
        "freqs = {}\n",
        "for word in words:\n",
        "    if word in freqs:\n",
        "        freqs[word] += 1\n",
        "    else:\n",
        "        freqs[word] = 1\n",
        "\n",
        "# Print results\n",
        "print(\"Word frequencies:\", freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUP46p5vtGyd",
        "outputId": "5a21a6aa-71b0-4c42-a30f-bddfbda126bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequencies: {'In_': 1, 'today’s_': 1, 'digital_': 1, 'world,_': 1, 'the_': 3, 'amount_': 1, 'of_': 2, 'textual_': 1, 'data_': 2, 'generated_': 1, 'every_': 1, 'second_': 1, 'is_': 3, 'staggering._': 1, 'From_': 1, 'social_': 1, 'media_': 1, 'updates_': 1, 'and_': 7, 'customer_': 3, 'reviews_': 1, 'to_': 4, 'news_': 1, 'articles_': 1, 'research_': 1, 'papers,_': 1, 'unstructured_': 1, 'text_': 1, 'everywhere._': 1, 'Natural_': 1, 'Language_': 1, 'Processing,_': 1, 'commonly_': 1, 'known_': 1, 'as_': 1, 'NLP,_': 1, 'plays_': 1, 'a_': 1, 'crucial_': 1, 'role_': 1, 'in_': 3, 'transforming_': 1, 'this_': 1, 'raw_': 1, 'into_': 1, 'meaningful_': 1, 'insights._': 1, 'Businesses_': 1, 'use_': 1, 'NLP_': 4, 'analyze_': 1, 'sentiment,_': 1, 'automate_': 1, 'support_': 1, 'services,_': 1, 'detect_': 1, 'trends_': 1, 'market_': 1, 'behavior._': 1, 'For_': 1, 'example,_': 1, 'sentiment_': 1, 'analysis_': 1, 'helps_': 1, 'companies_': 1, 'understand_': 1, 'how_': 1, 'their_': 1, 'products_': 1, 'are_': 2, 'perceived_': 1, 'by_': 2, 'users,_': 1, 'while_': 1, 'chatbots_': 1, 'powered_': 1, 'provide_': 1, 'instant_': 1, 'assistance_': 1, 'without_': 1, 'human_': 3, 'intervention._': 1, 'Despite_': 1, 'its_': 1, 'success,_': 1, 'still_': 1, 'faces_': 1, 'challenges_': 1, 'when_': 1, 'it_': 1, 'comes_': 1, 'context_': 1, 'understanding,_': 1, 'irony,_': 1, 'cultural_': 1, 'nuances,_': 1, 'multilingual_': 1, 'support._': 1, 'With_': 1, 'rapid_': 1, 'advancements_': 1, 'machine_': 2, 'learning_': 1, 'deep_': 1, 'learning,_': 1, 'however,_': 1, 'constantly_': 1, 'evolving,_': 1, 'making_': 1, 'machines_': 1, 'more_': 2, 'capable_': 1, 'truly_': 1, 'understanding_': 1, 'language._': 1, 'Researchers_': 1, 'working_': 1, 'on_': 1, 'large_': 1, 'language_': 1, 'models,_': 1, 'better_': 1, 'datasets,_': 1, 'accurate_': 1, 'algorithms_': 1, 'bridge_': 1, 'gap_': 1, 'between_': 1, 'communication._': 1, 'History._': 1, 'Generous._': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=[]\n",
        "for word in words:\n",
        "  for letter in word:\n",
        "    if letter not in vocab:\n",
        "      vocab.append(letter)\n",
        "vocab.sort()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ksXb2tXv38U",
        "outputId": "5a68dd7a-78ee-45e0-fe12-28e0fddb1dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',',\n",
              " '.',\n",
              " 'B',\n",
              " 'D',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'L',\n",
              " 'N',\n",
              " 'P',\n",
              " 'R',\n",
              " 'W',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '’']"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {word: [c for c in word] for word in freqs.keys()}\n",
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Xk9nz7xt8B",
        "outputId": "565d02d6-7d74-4907-826b-a002b0af04f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'In_': ['I', 'n', '_'],\n",
              " 'today’s_': ['t', 'o', 'd', 'a', 'y', '’', 's', '_'],\n",
              " 'digital_': ['d', 'i', 'g', 'i', 't', 'a', 'l', '_'],\n",
              " 'world,_': ['w', 'o', 'r', 'l', 'd', ',', '_'],\n",
              " 'the_': ['t', 'h', 'e', '_'],\n",
              " 'amount_': ['a', 'm', 'o', 'u', 'n', 't', '_'],\n",
              " 'of_': ['o', 'f', '_'],\n",
              " 'textual_': ['t', 'e', 'x', 't', 'u', 'a', 'l', '_'],\n",
              " 'data_': ['d', 'a', 't', 'a', '_'],\n",
              " 'generated_': ['g', 'e', 'n', 'e', 'r', 'a', 't', 'e', 'd', '_'],\n",
              " 'every_': ['e', 'v', 'e', 'r', 'y', '_'],\n",
              " 'second_': ['s', 'e', 'c', 'o', 'n', 'd', '_'],\n",
              " 'is_': ['i', 's', '_'],\n",
              " 'staggering._': ['s', 't', 'a', 'g', 'g', 'e', 'r', 'i', 'n', 'g', '.', '_'],\n",
              " 'From_': ['F', 'r', 'o', 'm', '_'],\n",
              " 'social_': ['s', 'o', 'c', 'i', 'a', 'l', '_'],\n",
              " 'media_': ['m', 'e', 'd', 'i', 'a', '_'],\n",
              " 'updates_': ['u', 'p', 'd', 'a', 't', 'e', 's', '_'],\n",
              " 'and_': ['a', 'n', 'd', '_'],\n",
              " 'customer_': ['c', 'u', 's', 't', 'o', 'm', 'e', 'r', '_'],\n",
              " 'reviews_': ['r', 'e', 'v', 'i', 'e', 'w', 's', '_'],\n",
              " 'to_': ['t', 'o', '_'],\n",
              " 'news_': ['n', 'e', 'w', 's', '_'],\n",
              " 'articles_': ['a', 'r', 't', 'i', 'c', 'l', 'e', 's', '_'],\n",
              " 'research_': ['r', 'e', 's', 'e', 'a', 'r', 'c', 'h', '_'],\n",
              " 'papers,_': ['p', 'a', 'p', 'e', 'r', 's', ',', '_'],\n",
              " 'unstructured_': ['u',\n",
              "  'n',\n",
              "  's',\n",
              "  't',\n",
              "  'r',\n",
              "  'u',\n",
              "  'c',\n",
              "  't',\n",
              "  'u',\n",
              "  'r',\n",
              "  'e',\n",
              "  'd',\n",
              "  '_'],\n",
              " 'text_': ['t', 'e', 'x', 't', '_'],\n",
              " 'everywhere._': ['e', 'v', 'e', 'r', 'y', 'w', 'h', 'e', 'r', 'e', '.', '_'],\n",
              " 'Natural_': ['N', 'a', 't', 'u', 'r', 'a', 'l', '_'],\n",
              " 'Language_': ['L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '_'],\n",
              " 'Processing,_': ['P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ',', '_'],\n",
              " 'commonly_': ['c', 'o', 'm', 'm', 'o', 'n', 'l', 'y', '_'],\n",
              " 'known_': ['k', 'n', 'o', 'w', 'n', '_'],\n",
              " 'as_': ['a', 's', '_'],\n",
              " 'NLP,_': ['N', 'L', 'P', ',', '_'],\n",
              " 'plays_': ['p', 'l', 'a', 'y', 's', '_'],\n",
              " 'a_': ['a', '_'],\n",
              " 'crucial_': ['c', 'r', 'u', 'c', 'i', 'a', 'l', '_'],\n",
              " 'role_': ['r', 'o', 'l', 'e', '_'],\n",
              " 'in_': ['i', 'n', '_'],\n",
              " 'transforming_': ['t',\n",
              "  'r',\n",
              "  'a',\n",
              "  'n',\n",
              "  's',\n",
              "  'f',\n",
              "  'o',\n",
              "  'r',\n",
              "  'm',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  '_'],\n",
              " 'this_': ['t', 'h', 'i', 's', '_'],\n",
              " 'raw_': ['r', 'a', 'w', '_'],\n",
              " 'into_': ['i', 'n', 't', 'o', '_'],\n",
              " 'meaningful_': ['m', 'e', 'a', 'n', 'i', 'n', 'g', 'f', 'u', 'l', '_'],\n",
              " 'insights._': ['i', 'n', 's', 'i', 'g', 'h', 't', 's', '.', '_'],\n",
              " 'Businesses_': ['B', 'u', 's', 'i', 'n', 'e', 's', 's', 'e', 's', '_'],\n",
              " 'use_': ['u', 's', 'e', '_'],\n",
              " 'NLP_': ['N', 'L', 'P', '_'],\n",
              " 'analyze_': ['a', 'n', 'a', 'l', 'y', 'z', 'e', '_'],\n",
              " 'sentiment,_': ['s', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', ',', '_'],\n",
              " 'automate_': ['a', 'u', 't', 'o', 'm', 'a', 't', 'e', '_'],\n",
              " 'support_': ['s', 'u', 'p', 'p', 'o', 'r', 't', '_'],\n",
              " 'services,_': ['s', 'e', 'r', 'v', 'i', 'c', 'e', 's', ',', '_'],\n",
              " 'detect_': ['d', 'e', 't', 'e', 'c', 't', '_'],\n",
              " 'trends_': ['t', 'r', 'e', 'n', 'd', 's', '_'],\n",
              " 'market_': ['m', 'a', 'r', 'k', 'e', 't', '_'],\n",
              " 'behavior._': ['b', 'e', 'h', 'a', 'v', 'i', 'o', 'r', '.', '_'],\n",
              " 'For_': ['F', 'o', 'r', '_'],\n",
              " 'example,_': ['e', 'x', 'a', 'm', 'p', 'l', 'e', ',', '_'],\n",
              " 'sentiment_': ['s', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', '_'],\n",
              " 'analysis_': ['a', 'n', 'a', 'l', 'y', 's', 'i', 's', '_'],\n",
              " 'helps_': ['h', 'e', 'l', 'p', 's', '_'],\n",
              " 'companies_': ['c', 'o', 'm', 'p', 'a', 'n', 'i', 'e', 's', '_'],\n",
              " 'understand_': ['u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', '_'],\n",
              " 'how_': ['h', 'o', 'w', '_'],\n",
              " 'their_': ['t', 'h', 'e', 'i', 'r', '_'],\n",
              " 'products_': ['p', 'r', 'o', 'd', 'u', 'c', 't', 's', '_'],\n",
              " 'are_': ['a', 'r', 'e', '_'],\n",
              " 'perceived_': ['p', 'e', 'r', 'c', 'e', 'i', 'v', 'e', 'd', '_'],\n",
              " 'by_': ['b', 'y', '_'],\n",
              " 'users,_': ['u', 's', 'e', 'r', 's', ',', '_'],\n",
              " 'while_': ['w', 'h', 'i', 'l', 'e', '_'],\n",
              " 'chatbots_': ['c', 'h', 'a', 't', 'b', 'o', 't', 's', '_'],\n",
              " 'powered_': ['p', 'o', 'w', 'e', 'r', 'e', 'd', '_'],\n",
              " 'provide_': ['p', 'r', 'o', 'v', 'i', 'd', 'e', '_'],\n",
              " 'instant_': ['i', 'n', 's', 't', 'a', 'n', 't', '_'],\n",
              " 'assistance_': ['a', 's', 's', 'i', 's', 't', 'a', 'n', 'c', 'e', '_'],\n",
              " 'without_': ['w', 'i', 't', 'h', 'o', 'u', 't', '_'],\n",
              " 'human_': ['h', 'u', 'm', 'a', 'n', '_'],\n",
              " 'intervention._': ['i',\n",
              "  'n',\n",
              "  't',\n",
              "  'e',\n",
              "  'r',\n",
              "  'v',\n",
              "  'e',\n",
              "  'n',\n",
              "  't',\n",
              "  'i',\n",
              "  'o',\n",
              "  'n',\n",
              "  '.',\n",
              "  '_'],\n",
              " 'Despite_': ['D', 'e', 's', 'p', 'i', 't', 'e', '_'],\n",
              " 'its_': ['i', 't', 's', '_'],\n",
              " 'success,_': ['s', 'u', 'c', 'c', 'e', 's', 's', ',', '_'],\n",
              " 'still_': ['s', 't', 'i', 'l', 'l', '_'],\n",
              " 'faces_': ['f', 'a', 'c', 'e', 's', '_'],\n",
              " 'challenges_': ['c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', '_'],\n",
              " 'when_': ['w', 'h', 'e', 'n', '_'],\n",
              " 'it_': ['i', 't', '_'],\n",
              " 'comes_': ['c', 'o', 'm', 'e', 's', '_'],\n",
              " 'context_': ['c', 'o', 'n', 't', 'e', 'x', 't', '_'],\n",
              " 'understanding,_': ['u',\n",
              "  'n',\n",
              "  'd',\n",
              "  'e',\n",
              "  'r',\n",
              "  's',\n",
              "  't',\n",
              "  'a',\n",
              "  'n',\n",
              "  'd',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  ',',\n",
              "  '_'],\n",
              " 'irony,_': ['i', 'r', 'o', 'n', 'y', ',', '_'],\n",
              " 'cultural_': ['c', 'u', 'l', 't', 'u', 'r', 'a', 'l', '_'],\n",
              " 'nuances,_': ['n', 'u', 'a', 'n', 'c', 'e', 's', ',', '_'],\n",
              " 'multilingual_': ['m',\n",
              "  'u',\n",
              "  'l',\n",
              "  't',\n",
              "  'i',\n",
              "  'l',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  'u',\n",
              "  'a',\n",
              "  'l',\n",
              "  '_'],\n",
              " 'support._': ['s', 'u', 'p', 'p', 'o', 'r', 't', '.', '_'],\n",
              " 'With_': ['W', 'i', 't', 'h', '_'],\n",
              " 'rapid_': ['r', 'a', 'p', 'i', 'd', '_'],\n",
              " 'advancements_': ['a',\n",
              "  'd',\n",
              "  'v',\n",
              "  'a',\n",
              "  'n',\n",
              "  'c',\n",
              "  'e',\n",
              "  'm',\n",
              "  'e',\n",
              "  'n',\n",
              "  't',\n",
              "  's',\n",
              "  '_'],\n",
              " 'machine_': ['m', 'a', 'c', 'h', 'i', 'n', 'e', '_'],\n",
              " 'learning_': ['l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', '_'],\n",
              " 'deep_': ['d', 'e', 'e', 'p', '_'],\n",
              " 'learning,_': ['l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ',', '_'],\n",
              " 'however,_': ['h', 'o', 'w', 'e', 'v', 'e', 'r', ',', '_'],\n",
              " 'constantly_': ['c', 'o', 'n', 's', 't', 'a', 'n', 't', 'l', 'y', '_'],\n",
              " 'evolving,_': ['e', 'v', 'o', 'l', 'v', 'i', 'n', 'g', ',', '_'],\n",
              " 'making_': ['m', 'a', 'k', 'i', 'n', 'g', '_'],\n",
              " 'machines_': ['m', 'a', 'c', 'h', 'i', 'n', 'e', 's', '_'],\n",
              " 'more_': ['m', 'o', 'r', 'e', '_'],\n",
              " 'capable_': ['c', 'a', 'p', 'a', 'b', 'l', 'e', '_'],\n",
              " 'truly_': ['t', 'r', 'u', 'l', 'y', '_'],\n",
              " 'understanding_': ['u',\n",
              "  'n',\n",
              "  'd',\n",
              "  'e',\n",
              "  'r',\n",
              "  's',\n",
              "  't',\n",
              "  'a',\n",
              "  'n',\n",
              "  'd',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  '_'],\n",
              " 'language._': ['l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '.', '_'],\n",
              " 'Researchers_': ['R', 'e', 's', 'e', 'a', 'r', 'c', 'h', 'e', 'r', 's', '_'],\n",
              " 'working_': ['w', 'o', 'r', 'k', 'i', 'n', 'g', '_'],\n",
              " 'on_': ['o', 'n', '_'],\n",
              " 'large_': ['l', 'a', 'r', 'g', 'e', '_'],\n",
              " 'language_': ['l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '_'],\n",
              " 'models,_': ['m', 'o', 'd', 'e', 'l', 's', ',', '_'],\n",
              " 'better_': ['b', 'e', 't', 't', 'e', 'r', '_'],\n",
              " 'datasets,_': ['d', 'a', 't', 'a', 's', 'e', 't', 's', ',', '_'],\n",
              " 'accurate_': ['a', 'c', 'c', 'u', 'r', 'a', 't', 'e', '_'],\n",
              " 'algorithms_': ['a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', '_'],\n",
              " 'bridge_': ['b', 'r', 'i', 'd', 'g', 'e', '_'],\n",
              " 'gap_': ['g', 'a', 'p', '_'],\n",
              " 'between_': ['b', 'e', 't', 'w', 'e', 'e', 'n', '_'],\n",
              " 'communication._': ['c',\n",
              "  'o',\n",
              "  'm',\n",
              "  'm',\n",
              "  'u',\n",
              "  'n',\n",
              "  'i',\n",
              "  'c',\n",
              "  'a',\n",
              "  't',\n",
              "  'i',\n",
              "  'o',\n",
              "  'n',\n",
              "  '.',\n",
              "  '_'],\n",
              " 'History._': ['H', 'i', 's', 't', 'o', 'r', 'y', '.', '_'],\n",
              " 'Generous._': ['G', 'e', 'n', 'e', 'r', 'o', 'u', 's', '.', '_']}"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPvdHymy1cXV",
        "outputId": "b42a3452-05b6-41a5-ce0e-a39ec2fffdf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    # pair_freqs = 0\n",
        "    for word, freq in freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs"
      ],
      "metadata": {
        "id": "dCuOM95HyV2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "print(pair_freqs)\n",
        "# for i, key in enumerate(pair_freqs.keys()):\n",
        "#     print(f\"{key}: {pair_freqs[key]}\")\n",
        "#     if i >= 5:\n",
        "#         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GgG2rYL089J",
        "outputId": "c30b040b-df0c-4f9e-bd65-c67240b3cd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {('I', 'n'): 1, ('n', '_'): 11, ('t', 'o'): 11, ('o', 'd'): 3, ('d', 'a'): 5, ('a', 'y'): 2, ('y', '’'): 1, ('’', 's'): 1, ('s', '_'): 26, ('d', 'i'): 4, ('i', 'g'): 2, ('g', 'i'): 1, ('i', 't'): 7, ('t', 'a'): 11, ('a', 'l'): 11, ('l', '_'): 9, ('w', 'o'): 2, ('o', 'r'): 11, ('r', 'l'): 1, ('l', 'd'): 1, ('d', ','): 1, (',', '_'): 17, ('t', 'h'): 8, ('h', 'e'): 8, ('e', '_'): 23, ('a', 'm'): 2, ('m', 'o'): 5, ('o', 'u'): 3, ('u', 'n'): 6, ('n', 't'): 12, ('t', '_'): 10, ('o', 'f'): 2, ('f', '_'): 2, ('t', 'e'): 11, ('e', 'x'): 4, ('x', 't'): 3, ('t', 'u'): 4, ('u', 'a'): 6, ('a', 't'): 10, ('a', '_'): 4, ('g', 'e'): 8, ('e', 'n'): 12, ('n', 'e'): 7, ('e', 'r'): 21, ('r', 'a'): 7, ('e', 'd'): 5, ('d', '_'): 14, ('e', 'v'): 5, ('v', 'e'): 5, ('r', 'y'): 3, ('y', '_'): 6, ('s', 'e'): 10, ('e', 'c'): 2, ('c', 'o'): 7, ('o', 'n'): 8, ('n', 'd'): 15, ('i', 's'): 7, ('s', 't'): 13, ('a', 'g'): 4, ('g', 'g'): 1, ('r', 'i'): 3, ('i', 'n'): 23, ('n', 'g'): 16, ('g', '.'): 1, ('.', '_'): 10, ('F', 'r'): 1, ('r', 'o'): 7, ('o', 'm'): 9, ('m', '_'): 1, ('s', 'o'): 1, ('o', 'c'): 2, ('c', 'i'): 2, ('i', 'a'): 3, ('m', 'e'): 9, ('u', 'p'): 3, ('p', 'd'): 1, ('e', 's'): 16, ('a', 'n'): 26, ('c', 'u'): 5, ('u', 's'): 7, ('r', '_'): 6, ('r', 'e'): 10, ('v', 'i'): 5, ('i', 'e'): 2, ('e', 'w'): 2, ('w', 's'): 2, ('o', '_'): 5, ('a', 'r'): 9, ('r', 't'): 3, ('t', 'i'): 7, ('i', 'c'): 3, ('c', 'l'): 1, ('l', 'e'): 8, ('e', 'a'): 5, ('r', 'c'): 3, ('c', 'h'): 7, ('h', '_'): 2, ('p', 'a'): 3, ('a', 'p'): 4, ('p', 'e'): 2, ('r', 's'): 6, ('s', ','): 7, ('n', 's'): 5, ('t', 'r'): 4, ('r', 'u'): 3, ('u', 'c'): 4, ('c', 't'): 3, ('u', 'r'): 4, ('y', 'w'): 1, ('w', 'h'): 3, ('e', '.'): 2, ('N', 'a'): 1, ('L', 'a'): 1, ('g', 'u'): 4, ('P', 'r'): 1, ('c', 'e'): 8, ('s', 's'): 4, ('s', 'i'): 5, ('g', ','): 4, ('m', 'm'): 2, ('n', 'l'): 1, ('l', 'y'): 5, ('k', 'n'): 1, ('n', 'o'): 1, ('o', 'w'): 4, ('w', 'n'): 1, ('a', 's'): 3, ('N', 'L'): 5, ('L', 'P'): 5, ('P', ','): 1, ('p', 'l'): 2, ('l', 'a'): 4, ('y', 's'): 2, ('c', 'r'): 1, ('o', 'l'): 2, ('s', 'f'): 1, ('f', 'o'): 1, ('r', 'm'): 1, ('m', 'i'): 1, ('g', '_'): 5, ('h', 'i'): 5, ('a', 'w'): 1, ('w', '_'): 2, ('n', 'i'): 5, ('g', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 4, ('g', 'h'): 1, ('h', 't'): 1, ('t', 's'): 6, ('s', '.'): 2, ('B', 'u'): 1, ('P', '_'): 4, ('n', 'a'): 2, ('y', 'z'): 1, ('z', 'e'): 1, ('i', 'm'): 2, ('t', ','): 1, ('a', 'u'): 1, ('u', 't'): 2, ('m', 'a'): 9, ('s', 'u'): 3, ('p', 'p'): 2, ('p', 'o'): 3, ('r', 'v'): 2, ('d', 'e'): 7, ('e', 't'): 5, ('d', 's'): 1, ('r', 'k'): 2, ('k', 'e'): 1, ('b', 'e'): 3, ('e', 'h'): 1, ('h', 'a'): 3, ('a', 'v'): 1, ('i', 'o'): 3, ('r', '.'): 1, ('F', 'o'): 1, ('x', 'a'): 1, ('m', 'p'): 2, ('e', ','): 1, ('e', 'l'): 2, ('l', 'p'): 1, ('p', 's'): 1, ('h', 'o'): 3, ('e', 'i'): 2, ('i', 'r'): 2, ('p', 'r'): 2, ('d', 'u'): 1, ('i', 'v'): 1, ('b', 'y'): 2, ('i', 'l'): 3, ('t', 'b'): 1, ('b', 'o'): 1, ('o', 't'): 1, ('w', 'e'): 3, ('o', 'v'): 1, ('i', 'd'): 3, ('n', 'c'): 3, ('w', 'i'): 1, ('h', 'u'): 3, ('u', 'm'): 3, ('n', '.'): 2, ('D', 'e'): 1, ('s', 'p'): 1, ('p', 'i'): 2, ('c', 'c'): 2, ('l', 'l'): 2, ('f', 'a'): 1, ('a', 'c'): 5, ('n', 'y'): 1, ('y', ','): 1, ('l', 't'): 2, ('n', 'u'): 1, ('m', 'u'): 2, ('l', 'i'): 1, ('t', '.'): 1, ('W', 'i'): 1, ('a', 'd'): 1, ('d', 'v'): 1, ('v', 'a'): 1, ('e', 'm'): 1, ('r', 'n'): 2, ('e', 'e'): 2, ('e', 'p'): 1, ('p', '_'): 2, ('r', ','): 1, ('t', 'l'): 1, ('v', 'o'): 1, ('l', 'v'): 1, ('a', 'k'): 1, ('k', 'i'): 2, ('c', 'a'): 2, ('a', 'b'): 1, ('b', 'l'): 1, ('R', 'e'): 1, ('r', 'g'): 1, ('l', 's'): 1, ('t', 't'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('b', 'r'): 1, ('d', 'g'): 1, ('g', 'a'): 1, ('t', 'w'): 1, ('H', 'i'): 1, ('y', '.'): 1, ('G', 'e'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_freq = 0\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79jS0mDO1SyS",
        "outputId": "b1839631-6af9-4ce0-827e-90bc1f8759aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('s', '_') 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merges = {(\"t\", \"h\"): \"th\"}\n",
        "# vocab.append(\"th\")\n",
        "# merges"
      ],
      "metadata": {
        "id": "fyBj0IDi2aAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "NYgOhJ7v2clQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splits=merge_pair('t','h',splits)\n",
        "# print(splits['with_'])"
      ],
      "metadata": {
        "id": "iFLOrbM07sUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pair_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKnU4ZXlD3qo",
        "outputId": "b8a918ae-3df1-43ce-96fe-74d3a3f48bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {('I', 'n'): 1, ('n', '_'): 11, ('t', 'o'): 11, ('o', 'd'): 3, ('d', 'a'): 5, ('a', 'y'): 2, ('y', '’'): 1, ('’', 's'): 1, ('s', '_'): 26, ('d', 'i'): 4, ('i', 'g'): 2, ('g', 'i'): 1, ('i', 't'): 7, ('t', 'a'): 11, ('a', 'l'): 11, ('l', '_'): 9, ('w', 'o'): 2, ('o', 'r'): 11, ('r', 'l'): 1, ('l', 'd'): 1, ('d', ','): 1, (',', '_'): 17, ('t', 'h'): 8, ('h', 'e'): 8, ('e', '_'): 23, ('a', 'm'): 2, ('m', 'o'): 5, ('o', 'u'): 3, ('u', 'n'): 6, ('n', 't'): 12, ('t', '_'): 10, ('o', 'f'): 2, ('f', '_'): 2, ('t', 'e'): 11, ('e', 'x'): 4, ('x', 't'): 3, ('t', 'u'): 4, ('u', 'a'): 6, ('a', 't'): 10, ('a', '_'): 4, ('g', 'e'): 8, ('e', 'n'): 12, ('n', 'e'): 7, ('e', 'r'): 21, ('r', 'a'): 7, ('e', 'd'): 5, ('d', '_'): 14, ('e', 'v'): 5, ('v', 'e'): 5, ('r', 'y'): 3, ('y', '_'): 6, ('s', 'e'): 10, ('e', 'c'): 2, ('c', 'o'): 7, ('o', 'n'): 8, ('n', 'd'): 15, ('i', 's'): 7, ('s', 't'): 13, ('a', 'g'): 4, ('g', 'g'): 1, ('r', 'i'): 3, ('i', 'n'): 23, ('n', 'g'): 16, ('g', '.'): 1, ('.', '_'): 10, ('F', 'r'): 1, ('r', 'o'): 7, ('o', 'm'): 9, ('m', '_'): 1, ('s', 'o'): 1, ('o', 'c'): 2, ('c', 'i'): 2, ('i', 'a'): 3, ('m', 'e'): 9, ('u', 'p'): 3, ('p', 'd'): 1, ('e', 's'): 16, ('a', 'n'): 26, ('c', 'u'): 5, ('u', 's'): 7, ('r', '_'): 6, ('r', 'e'): 10, ('v', 'i'): 5, ('i', 'e'): 2, ('e', 'w'): 2, ('w', 's'): 2, ('o', '_'): 5, ('a', 'r'): 9, ('r', 't'): 3, ('t', 'i'): 7, ('i', 'c'): 3, ('c', 'l'): 1, ('l', 'e'): 8, ('e', 'a'): 5, ('r', 'c'): 3, ('c', 'h'): 7, ('h', '_'): 2, ('p', 'a'): 3, ('a', 'p'): 4, ('p', 'e'): 2, ('r', 's'): 6, ('s', ','): 7, ('n', 's'): 5, ('t', 'r'): 4, ('r', 'u'): 3, ('u', 'c'): 4, ('c', 't'): 3, ('u', 'r'): 4, ('y', 'w'): 1, ('w', 'h'): 3, ('e', '.'): 2, ('N', 'a'): 1, ('L', 'a'): 1, ('g', 'u'): 4, ('P', 'r'): 1, ('c', 'e'): 8, ('s', 's'): 4, ('s', 'i'): 5, ('g', ','): 4, ('m', 'm'): 2, ('n', 'l'): 1, ('l', 'y'): 5, ('k', 'n'): 1, ('n', 'o'): 1, ('o', 'w'): 4, ('w', 'n'): 1, ('a', 's'): 3, ('N', 'L'): 5, ('L', 'P'): 5, ('P', ','): 1, ('p', 'l'): 2, ('l', 'a'): 4, ('y', 's'): 2, ('c', 'r'): 1, ('o', 'l'): 2, ('s', 'f'): 1, ('f', 'o'): 1, ('r', 'm'): 1, ('m', 'i'): 1, ('g', '_'): 5, ('h', 'i'): 5, ('a', 'w'): 1, ('w', '_'): 2, ('n', 'i'): 5, ('g', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 4, ('g', 'h'): 1, ('h', 't'): 1, ('t', 's'): 6, ('s', '.'): 2, ('B', 'u'): 1, ('P', '_'): 4, ('n', 'a'): 2, ('y', 'z'): 1, ('z', 'e'): 1, ('i', 'm'): 2, ('t', ','): 1, ('a', 'u'): 1, ('u', 't'): 2, ('m', 'a'): 9, ('s', 'u'): 3, ('p', 'p'): 2, ('p', 'o'): 3, ('r', 'v'): 2, ('d', 'e'): 7, ('e', 't'): 5, ('d', 's'): 1, ('r', 'k'): 2, ('k', 'e'): 1, ('b', 'e'): 3, ('e', 'h'): 1, ('h', 'a'): 3, ('a', 'v'): 1, ('i', 'o'): 3, ('r', '.'): 1, ('F', 'o'): 1, ('x', 'a'): 1, ('m', 'p'): 2, ('e', ','): 1, ('e', 'l'): 2, ('l', 'p'): 1, ('p', 's'): 1, ('h', 'o'): 3, ('e', 'i'): 2, ('i', 'r'): 2, ('p', 'r'): 2, ('d', 'u'): 1, ('i', 'v'): 1, ('b', 'y'): 2, ('i', 'l'): 3, ('t', 'b'): 1, ('b', 'o'): 1, ('o', 't'): 1, ('w', 'e'): 3, ('o', 'v'): 1, ('i', 'd'): 3, ('n', 'c'): 3, ('w', 'i'): 1, ('h', 'u'): 3, ('u', 'm'): 3, ('n', '.'): 2, ('D', 'e'): 1, ('s', 'p'): 1, ('p', 'i'): 2, ('c', 'c'): 2, ('l', 'l'): 2, ('f', 'a'): 1, ('a', 'c'): 5, ('n', 'y'): 1, ('y', ','): 1, ('l', 't'): 2, ('n', 'u'): 1, ('m', 'u'): 2, ('l', 'i'): 1, ('t', '.'): 1, ('W', 'i'): 1, ('a', 'd'): 1, ('d', 'v'): 1, ('v', 'a'): 1, ('e', 'm'): 1, ('r', 'n'): 2, ('e', 'e'): 2, ('e', 'p'): 1, ('p', '_'): 2, ('r', ','): 1, ('t', 'l'): 1, ('v', 'o'): 1, ('l', 'v'): 1, ('a', 'k'): 1, ('k', 'i'): 2, ('c', 'a'): 2, ('a', 'b'): 1, ('b', 'l'): 1, ('R', 'e'): 1, ('r', 'g'): 1, ('l', 's'): 1, ('t', 't'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('b', 'r'): 1, ('d', 'g'): 1, ('g', 'a'): 1, ('t', 'w'): 1, ('H', 'i'): 1, ('y', '.'): 1, ('G', 'e'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 179\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = 1\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ],
      "metadata": {
        "id": "Us7hL9PG2ngh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k8E98Y_EvJX",
        "outputId": "339991bb-4bf6-40f5-b4e1-d794d3d1e51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('t', 'h'): 'th', ('s', '_'): 's_', ('a', 'n'): 'an', ('e', '_'): 'e_', ('i', 'n'): 'in', ('e', 'r'): 'er', (',', '_'): ',_', ('d', '_'): 'd_', ('s', 't'): 'st', ('e', 'n'): 'en', ('in', 'g'): 'ing', ('a', 'l'): 'al', ('o', 'r'): 'or', ('t', '_'): 't_', ('a', 't'): 'at', ('.', '_'): '._', ('o', 'm'): 'om', ('a', 'r'): 'ar', ('o', 'n'): 'on', ('e', 's_'): 'es_', ('an', 'd_'): 'and_', ('e', 's'): 'es', ('al', '_'): 'al_', ('c', 'h'): 'ch', ('t', 'o'): 'to', ('u', 'n'): 'un', ('y', '_'): 'y_', ('e', 'v'): 'ev', ('i', 's_'): 'is_', ('c', 'u'): 'cu', ('to', '_'): 'to_', ('t', 'i'): 'ti', ('s', ',_'): 's,_', ('N', 'L'): 'NL', ('NL', 'P'): 'NLP', ('ing', '_'): 'ing_', ('st', 'an'): 'stan', ('t', 'e'): 'te', ('d', 'at'): 'dat', ('a', '_'): 'a_', ('e', 'd_'): 'ed_', ('a', 'g'): 'ag', ('er', '_'): 'er_', ('e', 'ar'): 'ear', ('a', 'p'): 'ap', ('u', 'c'): 'uc', ('r', 'o'): 'ro', ('c', 'es'): 'ces', ('ing', ',_'): 'ing,_', ('c', 'om'): 'com', ('o', 'w'): 'ow', ('u', 's'): 'us', ('NLP', '_'): 'NLP_', ('t', 's_'): 'ts_', ('m', 'a'): 'ma', ('th', 'e_'): 'the_', ('te', 'x'): 'tex', ('t', 'u'): 'tu', ('ev', 'er'): 'ever', ('c', 'on'): 'con', ('u', 'p'): 'up', ('cu', 'st'): 'cust', ('cust', 'om'): 'custom', ('custom', 'er_'): 'customer_', ('w', 'h'): 'wh', ('an', 'g'): 'ang', ('ang', 'u'): 'angu', ('angu', 'ag'): 'anguag', ('l', 'y_'): 'ly_', ('l', 'e_'): 'le_', ('in', '_'): 'in_', ('t', 'r'): 'tr', ('en', 'ti'): 'enti', ('m', 'en'): 'men', ('v', 'i'): 'vi', ('d', 'e'): 'de', ('b', 'e'): 'be', ('un', 'd'): 'und', ('und', 'er'): 'under', ('i', 'th'): 'ith', ('h', 'u'): 'hu', ('hu', 'm'): 'hum', ('hum', 'an'): 'human', ('human', '_'): 'human_', ('ma', 'ch'): 'mach', ('mach', 'in'): 'machin', ('n', '_'): 'n_', ('a', 'y'): 'ay', ('d', 'i'): 'di', ('i', 't'): 'it', ('w', 'or'): 'wor', ('a', 'm'): 'am', ('o', 'f'): 'of', ('of', '_'): 'of_', ('dat', 'a_'): 'data_', ('en', 'er'): 'ener', ('s', 'e'): 'se', ('i', 'al_'): 'ial_', ('m', 'e'): 'me', ('e', 'w'): 'ew', ('ew', 's_'): 'ews_', ('es', 'ear'): 'esear', ('esear', 'ch'): 'esearch', ('er', 's,_'): 'ers,_', ('r', 'uc'): 'ruc', ('tu', 'r'): 'tur', ('tex', 't_'): 'text_', ('e', '._'): 'e._', ('anguag', 'e_'): 'anguage_', ('com', 'm'): 'comm', ('p', 'l'): 'pl', ('u', 'l'): 'ul', ('s', 'i'): 'si', ('an', 'al'): 'anal', ('anal', 'y'): 'analy', ('s', 'enti'): 'senti', ('senti', 'men'): 'sentimen', ('at', 'e_'): 'ate_', ('s', 'up'): 'sup', ('sup', 'p'): 'supp', ('supp', 'or'): 'suppor', ('ces', ',_'): 'ces,_', ('h', 'ow'): 'how', ('e', 'i'): 'ei', ('p', 'ro'): 'pro', ('ar', 'e_'): 'are_', ('b', 'y_'): 'by_', ('on', '._'): 'on._', ('a', 'c'): 'ac', ('en', '_'): 'en_', ('under', 'stan'): 'understan', ('understan', 'd'): 'understand', ('machin', 'e_'): 'machine_', ('l', 'ear'): 'lear', ('lear', 'n'): 'learn', ('k', 'ing_'): 'king_', ('m', 'or'): 'mor', ('mor', 'e_'): 'more_', ('g', 'e_'): 'ge_', ('be', 't'): 'bet'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtFpWLqmEwpl",
        "outputId": "cc6d69fa-8c38-4664-9b65-75159755d221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'In_': ['I', 'n_'], 'today’s_': ['to', 'd', 'ay', '’', 's_'], 'digital_': ['di', 'g', 'it', 'al_'], 'world,_': ['wor', 'l', 'd', ',_'], 'the_': ['the_'], 'amount_': ['am', 'o', 'un', 't_'], 'of_': ['of_'], 'textual_': ['tex', 'tu', 'al_'], 'data_': ['data_'], 'generated_': ['g', 'ener', 'at', 'ed_'], 'every_': ['ever', 'y_'], 'second_': ['se', 'con', 'd_'], 'is_': ['is_'], 'staggering._': ['st', 'ag', 'g', 'er', 'ing', '._'], 'From_': ['F', 'r', 'om', '_'], 'social_': ['s', 'o', 'c', 'ial_'], 'media_': ['me', 'di', 'a_'], 'updates_': ['up', 'dat', 'es_'], 'and_': ['and_'], 'customer_': ['customer_'], 'reviews_': ['r', 'ev', 'i', 'ews_'], 'to_': ['to_'], 'news_': ['n', 'ews_'], 'articles_': ['ar', 'ti', 'c', 'l', 'es_'], 'research_': ['r', 'esearch', '_'], 'papers,_': ['p', 'ap', 'ers,_'], 'unstructured_': ['un', 'st', 'ruc', 'tur', 'ed_'], 'text_': ['text_'], 'everywhere._': ['ever', 'y', 'wh', 'er', 'e._'], 'Natural_': ['N', 'at', 'u', 'r', 'al_'], 'Language_': ['L', 'anguage_'], 'Processing,_': ['P', 'ro', 'ces', 's', 'ing,_'], 'commonly_': ['comm', 'on', 'ly_'], 'known_': ['k', 'n', 'ow', 'n_'], 'as_': ['a', 's_'], 'NLP,_': ['NLP', ',_'], 'plays_': ['pl', 'ay', 's_'], 'a_': ['a_'], 'crucial_': ['c', 'ruc', 'ial_'], 'role_': ['ro', 'le_'], 'in_': ['in_'], 'transforming_': ['tr', 'an', 's', 'f', 'or', 'm', 'ing_'], 'this_': ['th', 'is_'], 'raw_': ['r', 'a', 'w', '_'], 'into_': ['in', 'to_'], 'meaningful_': ['me', 'an', 'ing', 'f', 'ul', '_'], 'insights._': ['in', 'si', 'g', 'h', 't', 's', '._'], 'Businesses_': ['B', 'us', 'in', 'es', 's', 'es_'], 'use_': ['us', 'e_'], 'NLP_': ['NLP_'], 'analyze_': ['analy', 'z', 'e_'], 'sentiment,_': ['sentimen', 't', ',_'], 'automate_': ['a', 'u', 't', 'om', 'ate_'], 'support_': ['suppor', 't_'], 'services,_': ['s', 'er', 'vi', 'ces,_'], 'detect_': ['de', 'te', 'c', 't_'], 'trends_': ['tr', 'en', 'd', 's_'], 'market_': ['m', 'ar', 'k', 'e', 't_'], 'behavior._': ['be', 'h', 'a', 'vi', 'or', '._'], 'For_': ['F', 'or', '_'], 'example,_': ['e', 'x', 'am', 'pl', 'e', ',_'], 'sentiment_': ['sentimen', 't_'], 'analysis_': ['analy', 's', 'is_'], 'helps_': ['h', 'e', 'l', 'p', 's_'], 'companies_': ['com', 'p', 'an', 'i', 'es_'], 'understand_': ['under', 'st', 'and_'], 'how_': ['how', '_'], 'their_': ['th', 'ei', 'r', '_'], 'products_': ['pro', 'd', 'uc', 'ts_'], 'are_': ['are_'], 'perceived_': ['p', 'er', 'c', 'ei', 'v', 'ed_'], 'by_': ['by_'], 'users,_': ['us', 'ers,_'], 'while_': ['wh', 'i', 'le_'], 'chatbots_': ['ch', 'at', 'b', 'o', 'ts_'], 'powered_': ['p', 'ow', 'er', 'ed_'], 'provide_': ['pro', 'vi', 'd', 'e_'], 'instant_': ['in', 'stan', 't_'], 'assistance_': ['a', 's', 'si', 'stan', 'c', 'e_'], 'without_': ['w', 'ith', 'o', 'u', 't_'], 'human_': ['human_'], 'intervention._': ['in', 't', 'er', 'v', 'enti', 'on._'], 'Despite_': ['D', 'es', 'p', 'it', 'e_'], 'its_': ['i', 'ts_'], 'success,_': ['s', 'uc', 'ces', 's,_'], 'still_': ['st', 'i', 'l', 'l', '_'], 'faces_': ['f', 'ac', 'es_'], 'challenges_': ['ch', 'al', 'l', 'en', 'g', 'es_'], 'when_': ['wh', 'en_'], 'it_': ['i', 't_'], 'comes_': ['com', 'es_'], 'context_': ['con', 'text_'], 'understanding,_': ['understand', 'ing,_'], 'irony,_': ['i', 'r', 'on', 'y', ',_'], 'cultural_': ['cu', 'l', 'tur', 'al_'], 'nuances,_': ['n', 'u', 'an', 'ces,_'], 'multilingual_': ['m', 'ul', 'ti', 'l', 'ing', 'u', 'al_'], 'support._': ['suppor', 't', '._'], 'With_': ['W', 'ith', '_'], 'rapid_': ['r', 'ap', 'i', 'd_'], 'advancements_': ['a', 'd', 'v', 'an', 'c', 'e', 'men', 'ts_'], 'machine_': ['machine_'], 'learning_': ['learn', 'ing_'], 'deep_': ['de', 'e', 'p', '_'], 'learning,_': ['learn', 'ing,_'], 'however,_': ['how', 'ever', ',_'], 'constantly_': ['con', 'stan', 't', 'ly_'], 'evolving,_': ['ev', 'o', 'l', 'v', 'ing,_'], 'making_': ['ma', 'king_'], 'machines_': ['machin', 'es_'], 'more_': ['more_'], 'capable_': ['c', 'ap', 'a', 'b', 'le_'], 'truly_': ['tr', 'u', 'ly_'], 'understanding_': ['understand', 'ing_'], 'language._': ['l', 'anguag', 'e._'], 'Researchers_': ['R', 'esearch', 'er', 's_'], 'working_': ['wor', 'king_'], 'on_': ['on', '_'], 'large_': ['l', 'ar', 'ge_'], 'language_': ['l', 'anguage_'], 'models,_': ['m', 'o', 'de', 'l', 's,_'], 'better_': ['bet', 't', 'er_'], 'datasets,_': ['dat', 'a', 'se', 't', 's,_'], 'accurate_': ['ac', 'cu', 'r', 'ate_'], 'algorithms_': ['al', 'g', 'or', 'ith', 'm', 's_'], 'bridge_': ['b', 'r', 'i', 'd', 'ge_'], 'gap_': ['g', 'ap', '_'], 'between_': ['bet', 'w', 'e', 'en_'], 'communication._': ['comm', 'un', 'i', 'c', 'at', 'i', 'on._'], 'History._': ['H', 'i', 'st', 'or', 'y', '._'], 'Generous._': ['G', 'ener', 'o', 'us', '._']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    a=text.split()\n",
        "    words=[word+'_' for word in a]\n",
        "    freqs = Counter(words)\n",
        "    # pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    # pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    # splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    splits = {word: [c for c in word] for word in freqs.keys()}\n",
        "    for pair, merge in merges.items():\n",
        "        for word, split in splits.items(): # Changed line: Iterate over items instead of keys\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[word] = split # Changed line: Update the split for the corresponding word\n",
        "\n",
        "    return sum(splits.values(), []) # Changed line: Sum values instead of keys"
      ],
      "metadata": {
        "id": "2nrDqmxm3CY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=[tokenize(\"datas\")]\n",
        "sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5adw1laeHa7O",
        "outputId": "3c13050f-7d2b-4abd-b3e3-3288e3d6c4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dat', 'a', 's_']]"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RU3yRnbHdYD",
        "outputId": "c8361e05-81e4-4a55-a610-698e0b12ede2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 3. Train the Word2Vec model (Skip-Gram)\n",
        "model = Word2Vec(\n",
        "    sentences=sen,        # Input: list of tokenized sentences\n",
        "    vector_size=30,           # Embedding dimension\n",
        "    window=5,                  # Context window size\n",
        "    sg=1,                      # 1 for Skip-Gram, 0 for CBOW\n",
        "    min_count=1,               # Min word frequency to include\n",
        "    workers=4,                 # Number of CPU cores\n",
        "    negative=5,                # Number of negative samples\n",
        "    epochs=10                  # Training iterations\n",
        ")\n",
        "\n",
        "# 4. Access results\n",
        "# Get embedding for a word\n",
        "print(\"Embedding for 'dat':\", model.wv['dat'])\n",
        "# for token in sen:\n",
        "#   if token in model.wv:\n",
        "#     print(token, model.wv[token])\n",
        "# Find similar words\n",
        "print(\"Words similar to 'dat':\", model.wv.most_similar('dat', topn=5))\n",
        "\n",
        "# Compute similarity between two words\n",
        "print(\"Similarity between 'dat' and 'a':\", model.wv.similarity('dat', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b99065-7e4a-466c-a702-7c1b00055eda",
        "id": "aoylcd20OGGM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'dat': [-0.00525884  0.00107124 -0.0138021  -0.02560896 -0.00502669  0.00823265\n",
            " -0.00296009  0.01844554 -0.00914326  0.00753355  0.01818598  0.02781984\n",
            " -0.0048458  -0.03069381  0.01456851  0.00190595  0.02480636 -0.00271094\n",
            " -0.00879471 -0.0291767  -0.00285519  0.00942188  0.01800476  0.02350885\n",
            " -0.0190104   0.00619607  0.02029621 -0.0159935  -0.01035753  0.02265877]\n",
            "Words similar to 'dat': [('s_', 0.003974503837525845), ('a', -0.0643809512257576)]\n",
            "Similarity between 'dat' and 'a': -0.06438096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oF8Z0lzGOx7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}