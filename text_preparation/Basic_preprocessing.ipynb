{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d1ab591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8f42469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'F:\\NLP\\IMDB Dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb216ae",
   "metadata": {},
   "source": [
    "## 1. Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d13caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've seen lonesome dove, dead man's walk, and the streets of laredo, and now the return to lonesome dove. if you are hungry for more after watching lonesome dove, this'll fill yer belly. great cast, great story. most definitely a close second to lonesome dove. i will be purchasing this movie to add to my collection. this is the best, or at least my favorite performance by jon voight. he is captain call. lou gossett jr. playing isom pickett is not somebody i'd mess with, he is a bad ass with perspective. william peterson does a great job as well. rick schroder is back as newt with an angst filled performance that reminds me of his stint on nypd blue. my only problem with this film (and it's really picking nits) is that i had the impression that call wanted to be \"the first man to graze cattle in montana\", and it's obvious that dunnigan had already been there a while. a little inconsistent, but easily overlooked as you lose yourself in the fantastic tale. i especially love the apparent character growth of jasper fant and july johnson. i've watched this movie several times and am ready for another sequel.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][41641].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "84a29953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41649</th>\n",
       "      <td>was it a thriller, as i thought when i saw the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>joe d'amato might have made some other notable...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13419</th>\n",
       "      <td>there was nothing else on tv yesterday afterno...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "41649  was it a thriller, as i thought when i saw the...  negative\n",
       "4950   joe d'amato might have made some other notable...  negative\n",
       "13419  there was nothing else on tv yesterday afterno...  positive"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874630d3",
   "metadata": {},
   "source": [
    "## 2. HTML tag removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "08c8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def strip_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "161237f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df['review']:\n",
    "#     strip_html_tags(i)\n",
    "df['review']=df['review'].apply(strip_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a72059",
   "metadata": {},
   "source": [
    "## 3. URL removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2597d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "319420b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text With URL.\n",
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5ba8d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n"
     ]
    }
   ],
   "source": [
    "print(remove_URL(text1))\n",
    "print(remove_URL(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ca36c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63366b5f",
   "metadata": {},
   "source": [
    "## 4.Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ac55bb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string,time\n",
    "punc=string.punctuation\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "51e1ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in punc:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5207ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Tu hablas EspaÃ±ol\n",
      "                                                  review sentiment\n",
      "46238  you know im getting really tired of all the ge...  negative\n",
      "20884  great horror comedy from michael davisiwas lau...  positive\n",
      "29226  i have this movie on a collection of inexpensi...  positive\n",
      "1.6845834255218506 1744872371.4645138\n"
     ]
    }
   ],
   "source": [
    "sample_text='Hola! Tu hablas EspaÃ±ol?'\n",
    "start=time.time()\n",
    "print(remove_punc(sample_text))\n",
    "df['review']=df['review'].apply(remove_punc)\n",
    "print(df.sample(3))\n",
    "time1=time.time()-start\n",
    "print(time1, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e74d9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(text):\n",
    "    return text.translate(str.maketrans('','',punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "399bc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Tu hablas EspaÃ±ol\n",
      "                                                  review sentiment\n",
      "24498  theres some nice scenery to look at hereif you...  negative\n",
      "41068  i guess this would be a great movie for a true...  negative\n",
      "5442   im a huge fan of legendary director elia kazan...  positive\n",
      "1.2151718139648438 1744872373.1736803\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(sample_text))\n",
    "df['review']=df['review'].apply(remove_punc)\n",
    "print(df.sample(3))\n",
    "time2=time.time()-start\n",
    "print(time2, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a5b14425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3862923795322555\n"
     ]
    }
   ],
   "source": [
    "print(time1/time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cdd8f",
   "metadata": {},
   "source": [
    "## 5. ChatWords handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ef2b302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8727446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_chatwords(text):\n",
    "    new_text=[]\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words:\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return ' '.join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "787ad6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n"
     ]
    }
   ],
   "source": [
    "text = 'IMHO he is the best'\n",
    "print(convert_chatwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "aabe0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(convert_chatwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dec93",
   "metadata": {},
   "source": [
    "## 6. Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d0d67663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a good person\n",
      "<class 'textblob.blob.TextBlob'>\n",
      "He is a good person\n",
      "<class 'str'>\n",
      "He is a good person\n"
     ]
    }
   ],
   "source": [
    "text='He ys a goood prson'\n",
    "from textblob import TextBlob\n",
    "textblb=TextBlob(text)\n",
    "print(textblb.correct())\n",
    "print(type(textblb.correct()))\n",
    "print(textblb.correct().string)\n",
    "print(type(textblb.correct().string))\n",
    "text=textblb.correct().string\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "460f1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'person']\n"
     ]
    }
   ],
   "source": [
    "a=['goood','prson']\n",
    "for i in range(len(a)):\n",
    "    textblb=TextBlob(a[i])\n",
    "    a[i]=textblb.correct().string\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050f351",
   "metadata": {},
   "source": [
    "## 7. Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0db667af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c0a7cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwrds(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b8cf6c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words :probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probably all-time favorite movie, story selflessness, sacrifice dedication noble cause, preachy boring. never gets old, despite seen 15 times'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(f'Text With Stop Words :{text}')\n",
    "# Calling Function\n",
    "remove_stopwrds(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b69c78fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>oh dear gods awful stay away stay away think y...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17517</th>\n",
       "      <td>movie absolutely pointless one good esamples m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32608</th>\n",
       "      <td>movie right bad love war movies normally come ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "22221  oh dear gods awful stay away stay away think y...  negative\n",
       "17517  movie absolutely pointless one good esamples m...  negative\n",
       "32608  movie right bad love war movies normally come ...  negative"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].apply(remove_stopwrds)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514660e",
   "metadata": {},
   "source": [
    "## 8. Handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a6e90037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use The Regular Expressions to Remove the Emojies from Text or Whole Corpus.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3b4ddd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Python is '"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Texts \n",
    "text = \"Loved the movie. It was ðŸ˜˜\"\n",
    "text1 = 'Python is ðŸ”¥'\n",
    "# Remove Emojies using Fucntion\n",
    "print(remove_emoji(text))\n",
    "remove_emoji(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68878c",
   "metadata": {},
   "source": [
    "**Removing emoji and emojize the emoji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9d41e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n",
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize(text))\n",
    "print(emoji.demojize(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5c27d",
   "metadata": {},
   "source": [
    "## 9. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9f81396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola!', 'Mucho', 'gusto']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split method\n",
    "sen='Hola! Mucho gusto'\n",
    "sen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "09b2ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2168\\1874545485.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens=re.findall(\"[\\w']+\",sen)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hola', 'Mucho', 'gusto']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regular expression\n",
    "import re\n",
    "tokens=re.findall(\"[\\w']+\",sen)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "86b7fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I like to eat Guava, grapes.\n",
      "Meena's friends also like Guava.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "corpus=\"\"\"Hello! I like to eat Guava, grapes.\n",
    "Meena's friends also like Guava.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "407152d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'I like to eat Guava, grapes.', \"Meena's friends also like Guava.\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# Paragraph --> sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "19773402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "I like to eat Guava, grapes.\n",
      "Meena's friends also like Guava.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9cfe6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.', 'Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# paragraph --> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize   # 's is taken as one\n",
    "words=word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "597aede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!']\n",
      "['I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.']\n",
      "['Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence --> words\n",
    "for sentence in documents:\n",
    "  print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b76c6f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.', 'Meena', \"'\", 's', 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize # 's is taken as 2 tokens\n",
    "print(wordpunct_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "749d5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes.', 'Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer  # . is not treated separately, only last . is treated seprately\n",
    "tokenizer=TreebankWordTokenizer()  # it is a class\n",
    "print(tokenizer.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5efe00",
   "metadata": {},
   "source": [
    "**Applying on dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ecac9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens (text):\n",
    "    token=word_tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "34fa10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8965caf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, reviewers, mentioned, watching, 1, oz, e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thought, wonderful, way, spend, Tears, eyes, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, theres, family, little, boy, jake,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, matteis, love, Tears, eyes, money, vi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, reviewers, mentioned, watching, 1, oz, e...  positive\n",
       "1  [wonderful, little, production, filming, techn...  positive\n",
       "2  [thought, wonderful, way, spend, Tears, eyes, ...  positive\n",
       "3  [basically, theres, family, little, boy, jake,...  negative\n",
       "4  [petter, matteis, love, Tears, eyes, money, vi...  positive"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "700b48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# Some Sentences \n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "# Word Tokenize the Sentences\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e32932",
   "metadata": {},
   "source": [
    "**Limitation of nltk tokenizer**\n",
    "NLTK is Performing Well Altough it has some of issue , Like in above text u see it cannot handle the mail. But U can Use it Acording to the data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098716bf",
   "metadata": {},
   "source": [
    "#### 9.1 Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "633caf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0494e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're here to help! mail us at nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(nlp(sent6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a42ee115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(sent6)\n",
    "for token in doc1:\n",
    "    print(token.text)\n",
    "    # print(token)     -> same output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0a58d",
   "metadata": {},
   "source": [
    "## 10. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f1d08100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bd0238a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "Adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "history | histori\n"
     ]
    }
   ],
   "source": [
    "words=['Eating','eats','eat','ate','Adjustable','rafting','ability','meeting','history']\n",
    "for word in words:\n",
    "  print(word,'|',stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c930c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "congratul\n"
     ]
    }
   ],
   "source": [
    "#  disadvantage\n",
    "print(stemming.stem('history'))\n",
    "print(stemming.stem('congratulation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0eb094b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "ingeat\n"
     ]
    }
   ],
   "source": [
    "#  RegexStemmer class\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "print(reg_stemmer.stem('eating'))\n",
    "print(reg_stemmer.stem('ingeating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1ce5c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer2=RegexpStemmer('ing|s$|e$|able$',min=4)\n",
    "print(reg_stemmer2.stem('eating'))\n",
    "print(reg_stemmer2.stem('ingeating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "85ac90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "Adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "history | histori\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "  print(word,'|',snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8bafbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer--> gener,\tfairli,\tsportingli\n",
      "SnowballStemmer--> generous,\tfair,\tsport\n"
     ]
    }
   ],
   "source": [
    "# Difference between PorterStemmer and SnowballStemmer\n",
    "print('PorterStemmer--> '+ stemming.stem('Generous')+',\\t'+ stemming.stem('Fairly')+',\\t'+stemming.stem('sportingly'))\n",
    "print('SnowballStemmer--> '+ snowball_stemmer.stem('Generous')+',\\t'+ snowball_stemmer.stem('Fairly')+',\\t'+snowball_stemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4e4c663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making function to apply on dataset\n",
    "def Stemmerr(text_tokens):\n",
    "    return ' '.join([stemming.stem(word) for word in text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['review'].apply(Stemmerr)\n",
    "# not using , will use lemmatization as it is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a34138",
   "metadata": {},
   "source": [
    "## 11. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3e420a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet Lammatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "48af4cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8dc39dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n",
      "going\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('going',pos='n'))    #pos(parts of speech) tag--> noun\n",
    "print(lemmatizer.lemmatize('going',pos='v'))    #v-->verb\n",
    "print(lemmatizer.lemmatize('going',pos='a'))    #a-->adjective\n",
    "print(lemmatizer.lemmatize('going',pos='r'))    #r-->adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8fee1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | Eating\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "Adjustable | Adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meet\n",
      "history | history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word,'|',lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "951b9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# Sentence \n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Intilize Punctuation\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "# Tokenize Word\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Using a Loop to Remove Punctuations.\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "# Printing Word and Lemmatized Word\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2e293ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making function to apply on dataset\n",
    "def Lemma(text_tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4430d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [one, reviewer, mentioned, watching, 1, oz, ep...\n",
       "1        [wonderful, little, production, filming, techn...\n",
       "2        [thought, wonderful, way, spend, Tears, eye, h...\n",
       "3        [basically, there, family, little, boy, jake, ...\n",
       "4        [petter, matteis, love, Tears, eye, money, vis...\n",
       "                               ...                        \n",
       "49995    [thought, movie, right, good, job, wasnt, crea...\n",
       "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
       "49997    [catholic, taught, parochial, elementary, scho...\n",
       "49998    [im, going, disagree, previous, comment, side,...\n",
       "49999    [one, expects, star, trek, movie, high, art, f...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(Lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f19b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
