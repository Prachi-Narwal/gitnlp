{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1ab591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f42469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'F:\\NLP\\IMDB Dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb216ae",
   "metadata": {},
   "source": [
    "## 1. Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d13caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've seen lonesome dove, dead man's walk, and the streets of laredo, and now the return to lonesome dove. if you are hungry for more after watching lonesome dove, this'll fill yer belly. great cast, great story. most definitely a close second to lonesome dove. i will be purchasing this movie to add to my collection. this is the best, or at least my favorite performance by jon voight. he is captain call. lou gossett jr. playing isom pickett is not somebody i'd mess with, he is a bad ass with perspective. william peterson does a great job as well. rick schroder is back as newt with an angst filled performance that reminds me of his stint on nypd blue. my only problem with this film (and it's really picking nits) is that i had the impression that call wanted to be \"the first man to graze cattle in montana\", and it's obvious that dunnigan had already been there a while. a little inconsistent, but easily overlooked as you lose yourself in the fantastic tale. i especially love the apparent character growth of jasper fant and july johnson. i've watched this movie several times and am ready for another sequel.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][41641].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84a29953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>this film is a very descent remake of the famo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11071</th>\n",
       "      <td>a really cool flick. a must for any music snob...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>perfect movies are rare. even my favorite film...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "17992  this film is a very descent remake of the famo...  positive\n",
       "11071  a really cool flick. a must for any music snob...  positive\n",
       "714    perfect movies are rare. even my favorite film...  positive"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874630d3",
   "metadata": {},
   "source": [
    "## 2. HTML tag removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def strip_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "161237f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df['review']:\n",
    "#     strip_html_tags(i)\n",
    "df['review']=df['review'].apply(strip_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a72059",
   "metadata": {},
   "source": [
    "## 3. URL removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2597d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "319420b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text With URL.\n",
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba8d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n"
     ]
    }
   ],
   "source": [
    "print(remove_URL(text1))\n",
    "print(remove_URL(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca36c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63366b5f",
   "metadata": {},
   "source": [
    "## 4.Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac55bb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string,time\n",
    "punc=string.punctuation\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51e1ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in punc:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5207ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Tu hablas Español\n",
      "                                                  review sentiment\n",
      "12792  one of the most pleasant surprises of this ear...  negative\n",
      "45876  the progression of the plot is enough to rope ...  negative\n",
      "30199  legend of zui remember well tsui harks origina...  positive\n",
      "4.01327919960022 1744861634.395665\n"
     ]
    }
   ],
   "source": [
    "sample_text='Hola! Tu hablas Español?'\n",
    "start=time.time()\n",
    "print(remove_punc(sample_text))\n",
    "df['review']=df['review'].apply(remove_punc)\n",
    "print(df.sample(3))\n",
    "time1=time.time()-start\n",
    "print(time1, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e74d9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(text):\n",
    "    return text.translate(str.maketrans('','',punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "399bc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Tu hablas Español\n",
      "                                                  review sentiment\n",
      "13516  any movie that portrays the hardworking respon...  negative\n",
      "8934   mcboing boing is one of the cartoons that have...  positive\n",
      "19897  this is one of those horror flicks where twent...  negative\n",
      "3.250861883163452 1744861638.5200791\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(sample_text))\n",
    "df['review']=df['review'].apply(remove_punc)\n",
    "print(df.sample(3))\n",
    "time2=time.time()-start\n",
    "print(time2, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5b14425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.234527747975208\n"
     ]
    }
   ],
   "source": [
    "print(time1/time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cdd8f",
   "metadata": {},
   "source": [
    "## 5. ChatWords handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef2b302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8727446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_chatwords(text):\n",
    "    new_text=[]\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words:\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return ' '.join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "787ad6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n"
     ]
    }
   ],
   "source": [
    "text = 'IMHO he is the best'\n",
    "print(convert_chatwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aabe0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(convert_chatwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dec93",
   "metadata": {},
   "source": [
    "## 6. Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0d67663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a good person\n",
      "<class 'textblob.blob.TextBlob'>\n",
      "He is a good person\n",
      "<class 'str'>\n",
      "He is a good person\n"
     ]
    }
   ],
   "source": [
    "text='He ys a goood prson'\n",
    "from textblob import TextBlob\n",
    "textblb=TextBlob(text)\n",
    "print(textblb.correct())\n",
    "print(type(textblb.correct()))\n",
    "print(textblb.correct().string)\n",
    "print(type(textblb.correct().string))\n",
    "text=textblb.correct().string\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "460f1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'person']\n"
     ]
    }
   ],
   "source": [
    "a=['goood','prson']\n",
    "for i in range(len(a)):\n",
    "    textblb=TextBlob(a[i])\n",
    "    a[i]=textblb.correct().string\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050f351",
   "metadata": {},
   "source": [
    "## 7. Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0db667af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0a7cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwrds(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8cf6c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words :probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probably all-time favorite movie, story selflessness, sacrifice dedication noble cause, preachy boring. never gets old, despite seen 15 times'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(f'Text With Stop Words :{text}')\n",
    "# Calling Function\n",
    "remove_stopwrds(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b69c78fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33279</th>\n",
       "      <td>speechless devastated first viewing many parts...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37848</th>\n",
       "      <td>boston blackie movies strengths mostly pacing ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39064</th>\n",
       "      <td>first movie true facts saw documentary days ea...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "33279  speechless devastated first viewing many parts...  positive\n",
       "37848  boston blackie movies strengths mostly pacing ...  negative\n",
       "39064  first movie true facts saw documentary days ea...  negative"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].apply(remove_stopwrds)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514660e",
   "metadata": {},
   "source": [
    "## 8. Handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6e90037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use The Regular Expressions to Remove the Emojies from Text or Whole Corpus.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b4ddd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Python is '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Texts \n",
    "text = \"Loved the movie. It was 😘\"\n",
    "text1 = 'Python is 🔥'\n",
    "# Remove Emojies using Fucntion\n",
    "print(remove_emoji(text))\n",
    "remove_emoji(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68878c",
   "metadata": {},
   "source": [
    "**Removing emoji and emojize the emoji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d41e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n",
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize(text))\n",
    "print(emoji.demojize(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5c27d",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86b7fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I like to eat Guava, grapes.\n",
      "Meena's friends also like Guava.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "corpus=\"\"\"Hello! I like to eat Guava, grapes.\n",
    "Meena's friends also like Guava.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "407152d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'I like to eat Guava, grapes.', \"Meena's friends also like Guava.\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# Paragraph --> sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19773402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "I like to eat Guava, grapes.\n",
      "Meena's friends also like Guava.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9cfe6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.', 'Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# paragraph --> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize   # 's is taken as one\n",
    "words=word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "597aede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!']\n",
      "['I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.']\n",
      "['Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence --> words\n",
    "for sentence in documents:\n",
    "  print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b76c6f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes', '.', 'Meena', \"'\", 's', 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize # 's is taken as 2 tokens\n",
    "print(wordpunct_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "749d5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'like', 'to', 'eat', 'Guava', ',', 'grapes.', 'Meena', \"'s\", 'friends', 'also', 'like', 'Guava', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer  # . is not treated separately, only last . is treated seprately\n",
    "tokenizer=TreebankWordTokenizer()  # it is a class\n",
    "print(tokenizer.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5efe00",
   "metadata": {},
   "source": [
    "**Applying on dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecac9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens (text):\n",
    "    token=word_tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34fa10e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [one, reviewers, mentioned, watching, 1, oz, e...\n",
       "1        [wonderful, little, production, filming, techn...\n",
       "2        [thought, wonderful, way, spend, Tears, eyes, ...\n",
       "3        [basically, theres, family, little, boy, jake,...\n",
       "4        [petter, matteis, love, Tears, eyes, money, vi...\n",
       "                               ...                        \n",
       "49995    [thought, movie, right, good, job, wasnt, crea...\n",
       "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
       "49997    [catholic, taught, parochial, elementary, scho...\n",
       "49998    [im, going, disagree, previous, comment, side,...\n",
       "49999    [one, expects, star, trek, movies, high, art, ...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "700b48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# Some Sentences \n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "# Word Tokenize the Sentences\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e32932",
   "metadata": {},
   "source": [
    "**Limitation of nltk tokenizer**\n",
    "NLTK is Performing Well Altough it has some of issue , Like in above text u see it cannot handle the mail. But U can Use it Acording to the data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098716bf",
   "metadata": {},
   "source": [
    "#### 9.1 Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "633caf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0494e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're here to help! mail us at nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(nlp(sent6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ee115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(sent6)\n",
    "for token in doc1:\n",
    "    print(token.text)\n",
    "    # print(token)     -> same output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0a58d",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1d08100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd0238a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "Adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "history | histori\n"
     ]
    }
   ],
   "source": [
    "words=['Eating','eats','eat','ate','Adjustable','rafting','ability','meeting','history']\n",
    "for word in words:\n",
    "  print(word,'|',stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c930c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "congratul\n"
     ]
    }
   ],
   "source": [
    "#  disadvantage\n",
    "print(stemming.stem('history'))\n",
    "print(stemming.stem('congratulation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0eb094b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "ingeat\n"
     ]
    }
   ],
   "source": [
    "#  RegexStemmer class\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "print(reg_stemmer.stem('eating'))\n",
    "print(reg_stemmer.stem('ingeating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ce5c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer2=RegexpStemmer('ing|s$|e$|able$',min=4)\n",
    "print(reg_stemmer2.stem('eating'))\n",
    "print(reg_stemmer2.stem('ingeating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85ac90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "Adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "history | histori\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "  print(word,'|',snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bafbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer--> gener,\tfairli,\tsportingli\n",
      "SnowballStemmer--> generous,\tfair,\tsport\n"
     ]
    }
   ],
   "source": [
    "# Difference between PorterStemmer and SnowballStemmer\n",
    "print('PorterStemmer--> '+ stemming.stem('Generous')+',\\t'+ stemming.stem('Fairly')+',\\t'+stemming.stem('sportingly'))\n",
    "print('SnowballStemmer--> '+ snowball_stemmer.stem('Generous')+',\\t'+ snowball_stemmer.stem('Fairly')+',\\t'+snowball_stemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a34138",
   "metadata": {},
   "source": [
    "## 11. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e420a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet Lammatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48af4cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8dc39dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n",
      "going\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('going',pos='n'))    #pos(parts of speech) tag--> noun\n",
    "print(lemmatizer.lemmatize('going',pos='v'))    #v-->verb\n",
    "print(lemmatizer.lemmatize('going',pos='a'))    #a-->adjective\n",
    "print(lemmatizer.lemmatize('going',pos='r'))    #r-->adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8fee1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating | Eating\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "Adjustable | Adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meet\n",
      "history | history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word,'|',lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
